---
phase: 04-calibration-evaluation
plan: 02
type: execute
---

<objective>
Create evaluation framework with Brier scoring, calibration metrics, and human baseline comparison.

Purpose: Measure system performance against ground truth, detect calibration drift, and prove superiority to human forecasters.
Output: Complete evaluation system with metrics calculation, provisional scoring, and performance benchmarking.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-calibration-evaluation/04-CONTEXT.md
@.planning/phases/04-calibration-evaluation/04-RESEARCH.md
@.planning/phases/04-calibration-evaluation/04-01-SUMMARY.md
@src/calibration/prediction_store.py
@src/calibration/isotonic_calibrator.py
@src/forecasting/forecast_engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Brier scoring with provisional evaluation</name>
  <files>src/evaluation/brier_scorer.py, src/evaluation/__init__.py, src/evaluation/provisional_scorer.py</files>
  <action>Create BrierScorer class using sklearn.metrics.brier_score_loss for resolved predictions. Implement score_batch(predictions, outcomes) returning overall Brier score and per-category scores. Create ProvisionalScorer for unresolved predictions: calculate progress indicators from current GDELT events (tension index from QuadClass 4 event frequency, diplomatic index from QuadClass 1), apply time decay weight (1 - days_remaining/30), compute weighted provisional outcome. Include score_all() method combining resolved (weight=1.0) and provisional (weight=0.5) scores. Target Brier score <0.35 to beat human baseline. Log detailed scoring breakdowns.</action>
  <verify>Run scorer on 10 resolved + 5 provisional predictions, verify Brier score calculation matches manual check</verify>
  <done>Brier scoring implemented for resolved and provisional predictions, per-category scores available</done>
</task>

<task type="auto">
  <name>Task 2: Calculate calibration metrics (ECE, MCE) using netcal</name>
  <files>src/evaluation/calibration_metrics.py, src/evaluation/drift_detector.py</files>
  <action>Implement CalibrationMetrics class using netcal library. Calculate ECE (Expected Calibration Error) with 10 bins, MCE (Maximum Calibration Error), and ACE (Adaptive Calibration Error). Create reliability_diagram() method generating matplotlib plots showing predicted vs actual probabilities. Implement DriftDetector monitoring ECE over time using sliding 30-day window, alert when ECE > 0.15 (poor calibration). Add per-category metrics tracking. Store metrics history in SQLite for trend analysis. Generate automatic recalibration recommendations when drift detected.</action>
  <verify>Calculate ECE on 100+ predictions, generate reliability diagram, confirm ECE < 0.1 for calibrated predictions</verify>
  <done>ECE, MCE metrics calculated, reliability diagrams generated, drift detection operational</done>
</task>

<task type="auto">
  <name>Task 3: Build evaluation framework with human baseline comparison</name>
  <files>src/evaluation/benchmark.py, src/evaluation/evaluator.py, evaluate.py</files>
  <action>Create comprehensive Evaluator class orchestrating all metrics. Load predictions from SQLite, calculate Brier scores (target <0.35), calibration metrics (ECE target <0.1), generate performance report. Include human baseline comparison: expert forecasters typically achieve 0.35 Brier score, superforecasters 0.25. Create evaluate.py CLI script with commands: 'evaluate score' (current metrics), 'evaluate trend' (performance over time), 'evaluate calibrate' (trigger recalibration), 'evaluate report' (full HTML report). Add JSON export for metrics. Track improvement trajectory toward beating human baseline.</action>
  <verify>Run 'python evaluate.py score' and 'python evaluate.py report', verify outputs generated correctly</verify>
  <done>Evaluation framework complete, CLI working, metrics calculated, baseline comparison shown</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `uv run pytest tests/test_evaluation.py -v` passes all tests
- [ ] Brier score calculation working for resolved and provisional predictions
- [ ] ECE < 0.1 demonstrating good calibration
- [ ] Reliability diagrams generated correctly
- [ ] evaluate.py CLI produces comprehensive reports
- [ ] Performance tracking shows path to <0.35 Brier score
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Brier scoring with provisional evaluation working
- Calibration metrics (ECE, MCE) calculated via netcal
- Human baseline comparison framework operational
- Phase 4 complete
</success_criteria>

<output>
After completion, create `.planning/phases/04-calibration-evaluation/04-02-SUMMARY.md`:

# Phase 4 Plan 2: Evaluation Framework Summary

**Built comprehensive evaluation system with Brier scoring and human baseline comparison**

## Accomplishments

- Implemented Brier scoring for resolved and provisional predictions
- Created calibration metrics calculation using netcal (ECE, MCE, ACE)
- Built drift detection with automatic recalibration triggers
- Developed evaluation CLI with performance reporting
- Established human baseline comparison framework

## Files Created/Modified

- `src/evaluation/brier_scorer.py` - Brier score calculation
- `src/evaluation/provisional_scorer.py` - Scoring for unresolved predictions
- `src/evaluation/calibration_metrics.py` - ECE, MCE metrics via netcal
- `src/evaluation/drift_detector.py` - Temporal calibration drift monitoring
- `src/evaluation/benchmark.py` - Human baseline comparison
- `src/evaluation/evaluator.py` - Orchestration of all metrics
- `evaluate.py` - CLI for evaluation and reporting

## Decisions Made

- Weight provisional predictions at 0.5 vs resolved at 1.0
- 30-day sliding window for drift detection
- ECE > 0.15 triggers recalibration alert
- Target Brier < 0.35 to beat human experts

## Issues Encountered

[Document any issues that arose]

## Next Phase Readiness

Phase 4 (Calibration & Evaluation) complete! The system now has:
- Isotonic calibration with per-category curves
- Temperature scaling for confidence adjustment
- Comprehensive evaluation metrics
- Human baseline comparison
- Drift detection and recalibration triggers

Ready for production use or additional phases if defined.
</output>