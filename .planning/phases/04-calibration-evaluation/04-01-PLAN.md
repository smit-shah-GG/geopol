---
phase: 04-calibration-evaluation
plan: 01
type: execute
---

<objective>
Implement probability calibration system with isotonic regression and per-category curves.

Purpose: Transform raw model predictions into calibrated probabilities that accurately reflect true outcome likelihoods, enabling trustworthy confidence scores.
Output: Working calibration system with prediction tracking, isotonic calibration, and temperature scaling integration.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-calibration-evaluation/04-CONTEXT.md
@.planning/phases/04-calibration-evaluation/04-RESEARCH.md
@.planning/phases/03-hybrid-forecasting/03-04-SUMMARY.md
@src/forecasting/ensemble_predictor.py
@src/forecasting/forecast_engine.py
@src/forecasting/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create prediction tracking system with SQLite</name>
  <files>src/calibration/prediction_store.py, src/calibration/__init__.py</files>
  <action>Create SQLite-backed prediction storage system. Schema: predictions table with id (INTEGER PRIMARY KEY), query (TEXT), timestamp (DATETIME), raw_probability (REAL 0-1), calibrated_probability (REAL 0-1 nullable), category (TEXT - conflict/diplomatic/economic), entities (JSON), outcome (REAL 0-1 nullable), resolution_date (DATETIME nullable), metadata (JSON for scenarios/reasoning). Include methods: store_prediction(), update_outcome(), get_predictions_for_calibration(category=None, resolved_only=False), get_recent_predictions(days=30). Use SQLAlchemy for ORM to avoid SQL injection. Include indices on timestamp, category, and resolution_date for query performance.</action>
  <verify>Create test database, store 5 sample predictions, retrieve by category, verify indices exist</verify>
  <done>SQLite database created with predictions table, ORM models defined, CRUD operations working, indices present</done>
</task>

<task type="auto">
  <name>Task 2: Implement isotonic calibration with per-category curves</name>
  <files>src/calibration/isotonic_calibrator.py, src/calibration/explainer.py</files>
  <action>Implement isotonic calibration using scikit-learn CalibratedClassifierCV. Create IsotonicCalibrator class with fit(predictions, outcomes, categories) method that trains separate calibrators per category (conflict/diplomatic/economic). Use method='isotonic' for >1000 samples per category, fallback to 'sigmoid' for smaller datasets. Implement calibrate(probability, category) method returning calibrated probability. Add explainer module using calibrated-explanations to generate explanations like "Reduced confidence by 15% - similar Russia predictions overconfident in 8/10 recent cases". Store calibration curves in pickle files for persistence. Include recalibrate() method for periodic updates.</action>
  <verify>pytest test_calibration.py - tests isotonic fitting, per-category curves, explanation generation</verify>
  <done>Isotonic calibration working, separate curves per category, explanations generated, curves persisted</done>
</task>

<task type="auto">
  <name>Task 3: Integrate temperature scaling into ensemble predictor</name>
  <files>src/calibration/temperature_scaler.py, src/forecasting/ensemble_predictor.py</files>
  <action>Create TemperatureScaler class that learns optimal temperature T from validation data using log loss minimization. Implement fit(logits, labels) using scipy.optimize.minimize with L-BFGS to find optimal T. Add calibrate_confidence(confidence, temperature) method applying c' = c^(1/T). Modify EnsemblePredictor to: (1) Store raw logits alongside probabilities, (2) Apply learned temperature scaling after weighted voting, (3) Track both raw and calibrated confidence scores. Temperature should be category-specific. Default T=1.0 (no scaling) when insufficient calibration data.</action>
  <verify>Run forecast.py with --verbose, confirm calibrated_confidence differs from raw_confidence in output</verify>
  <done>Temperature scaling implemented, integrated with ensemble, calibrated confidence in output</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `uv run pytest tests/test_calibration.py -v` passes all tests
- [ ] Prediction storage working with 100+ sample predictions
- [ ] Isotonic calibration reduces ECE below 0.1
- [ ] Temperature scaling adjusts confidence appropriately
- [ ] Explanations generated for calibration adjustments
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Calibration system operational with explainable adjustments
- Per-category calibration curves trained
- Temperature scaling integrated
</success_criteria>

<output>
After completion, create `.planning/phases/04-calibration-evaluation/04-01-SUMMARY.md`:

# Phase 4 Plan 1: Probability Calibration Summary

**Implemented isotonic regression calibration with explainable adjustments**

## Accomplishments

- Created SQLite-backed prediction tracking system
- Implemented per-category isotonic calibration
- Integrated temperature scaling for confidence adjustment
- Added explainable calibration with adjustment reasons

## Files Created/Modified

- `src/calibration/prediction_store.py` - SQLite prediction tracking
- `src/calibration/isotonic_calibrator.py` - Per-category isotonic regression
- `src/calibration/temperature_scaler.py` - Temperature scaling optimization
- `src/calibration/explainer.py` - Calibration explanations
- `src/forecasting/ensemble_predictor.py` - Modified for temperature scaling

## Decisions Made

- Used SQLite for lightweight prediction tracking (no external DB needed)
- Separate calibration curves per event category for better accuracy
- Temperature scaling applied after ensemble voting

## Issues Encountered

[Document any issues that arose]

## Next Step

Ready for 04-02-PLAN.md - Evaluation framework implementation
</output>