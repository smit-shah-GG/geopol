---
phase: 03-hybrid-forecasting
plan: 02
type: execute
---

<objective>
Build RAG pipeline to ground LLM predictions in historical graph patterns.

Purpose: Convert temporal knowledge graph patterns into retrievable documents for Gemini context injection.
Output: Working RAG system that retrieves relevant historical precedents for scenario validation.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-hybrid-forecasting/03-RESEARCH.md
@.planning/phases/03-hybrid-forecasting/03-01-SUMMARY.md
@.planning/phases/02-knowledge-graph-engine/02-01-SUMMARY.md
@.planning/phases/02-knowledge-graph-engine/02-03-SUMMARY.md
@src/knowledge_graph/temporal_graph_builder.py
@src/knowledge_graph/query_engine.py
@src/forecasting/reasoning_orchestrator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract graph patterns as documents</name>
  <files>src/forecasting/graph_pattern_extractor.py</files>
  <action>Create GraphPatternExtractor class that converts temporal graph patterns into LlamaIndex documents. Extract 2-hop subgraphs around key entities, temporal patterns (recurring events, escalation sequences), and relationship evolution. Each pattern becomes a Document with text description and metadata (entities, timeframe, pattern_type, confidence). Use QueryEngine from Phase 2 to find patterns: bilateral relations, temporal sequences, k-hop neighborhoods. Format descriptions naturally: "USA and Russia showed diplomatic tensions in Q1 2024, followed by..." Store pattern confidence scores from graph statistics.</action>
  <verify>python -c "from src.forecasting.graph_pattern_extractor import GraphPatternExtractor; extractor = GraphPatternExtractor(); patterns = extractor.extract_patterns(); print(f'Extracted {len(patterns)} patterns')" runs without error</verify>
  <done>GraphPatternExtractor successfully converts graph patterns to retrievable documents</done>
</task>

<task type="auto">
  <name>Task 2: Set up LlamaIndex for pattern retrieval</name>
  <files>src/forecasting/rag_pipeline.py, requirements.txt</files>
  <action>Implement RAGPipeline using LlamaIndex. Create VectorStoreIndex from extracted graph patterns using local embeddings (sentence-transformers/all-MiniLM-L6-v2 for CPU efficiency). Configure SimpleNodeParser with chunk_size=512, chunk_overlap=50. Set up ChromaDB as local vector store for persistence. Implement hybrid search combining vector similarity and keyword matching for better recall. Add retrieval methods: retrieve_for_scenario(scenario) and retrieve_for_entities(entity_list). Add llama-index>=0.14, chromadb>=0.4, sentence-transformers>=2.5 to requirements.txt.</action>
  <verify>python -c "from src.forecasting.rag_pipeline import RAGPipeline; rag = RAGPipeline(); results = rag.retrieve_for_entities(['USA', 'CHN']); print(f'Retrieved {len(results)} documents')" returns results</verify>
  <done>RAGPipeline indexes graph patterns and retrieves relevant historical context</done>
</task>

<task type="auto">
  <name>Task 3: Integrate RAG with reasoning orchestrator</name>
  <files>src/forecasting/reasoning_orchestrator.py, src/forecasting/context_injector.py, tests/test_rag_integration.py</files>
  <action>Create ContextInjector class that bridges RAG pipeline with ReasoningOrchestrator. For each scenario generated by Gemini, retrieve relevant historical patterns and inject them as validation context. Implement prompt templates that incorporate retrieved patterns: "Historical precedent: {pattern_description} (confidence: {score})". Modify ReasoningOrchestrator to use ContextInjector in validation step. Limit context to top-5 most relevant patterns to avoid token overflow. Create integration test verifying that historical patterns influence scenario refinement.</action>
  <verify>python -m pytest tests/test_rag_integration.py -v passes all tests</verify>
  <done>RAG pipeline successfully grounds LLM predictions with historical patterns</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Graph patterns successfully extracted and indexed
- [ ] RAG retrieval returns relevant historical precedents
- [ ] Context injection preserves within token limits
- [ ] Retrieved patterns appear in reasoning chains
- [ ] Integration tests pass
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Graph patterns converted to searchable documents
- LlamaIndex RAG pipeline operational
- Historical grounding integrated with reasoning flow
- Retrieved context influences predictions
</success_criteria>

<output>
After completion, create `.planning/phases/03-hybrid-forecasting/03-02-SUMMARY.md`:

# Phase 3 Plan 2: RAG Pipeline Summary

**Built RAG system to ground predictions in historical graph patterns**

## Accomplishments

- Extracted temporal patterns from knowledge graph as documents
- Set up LlamaIndex with ChromaDB for efficient retrieval
- Integrated historical grounding into reasoning pipeline
- Implemented hybrid search for better recall

## Files Created/Modified

- `src/forecasting/graph_pattern_extractor.py` - Pattern extraction from TKG
- `src/forecasting/rag_pipeline.py` - LlamaIndex RAG implementation
- `src/forecasting/context_injector.py` - RAG-LLM integration
- `src/forecasting/reasoning_orchestrator.py` - Modified for RAG context
- `tests/test_rag_integration.py` - Integration tests
- `requirements.txt` - Added llama-index, chromadb, sentence-transformers

## Decisions Made

- Used LlamaIndex for RAG (better for prototyping than Haystack)
- ChromaDB for local vector storage (simpler than Qdrant for PoC)
- All-MiniLM-L6-v2 embeddings (CPU-friendly, good performance)
- 512 token chunks with 50 token overlap for optimal retrieval

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 03-03-PLAN.md (TKG algorithms integration)
</output>