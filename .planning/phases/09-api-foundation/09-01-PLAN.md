---
phase: 09-api-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - docker-compose.yml
  - Dockerfile
  - .env.example
  - src/settings.py
  - src/db/__init__.py
  - src/db/postgres.py
  - src/db/sqlite.py
  - src/db/models.py
  - alembic.ini
  - alembic/env.py
  - alembic/script.py.mako
  - alembic/versions/.gitkeep
autonomous: true

must_haves:
  truths:
    - "docker-compose up starts PostgreSQL 16 and Redis 7 with health checks passing"
    - "Alembic migration creates predictions, outcome_records, calibration_weights, ingest_runs, and api_keys tables in PostgreSQL"
    - "Async SQLAlchemy session connects to PostgreSQL via asyncpg with connection pooling"
    - "SQLite connection manager retains WAL mode with 30s busy_timeout for GDELT store"
    - "Application settings load from environment variables with sensible dev defaults"
  artifacts:
    - path: "src/db/models.py"
      provides: "SQLAlchemy ORM models for all PostgreSQL tables"
      contains: "class Prediction"
    - path: "src/db/postgres.py"
      provides: "Async engine, session factory, connection pool"
      contains: "create_async_engine"
    - path: "src/db/sqlite.py"
      provides: "SQLite connection manager with WAL mode"
      contains: "PRAGMA journal_mode = WAL"
    - path: "docker-compose.yml"
      provides: "PostgreSQL + Redis + API service definitions"
      contains: "postgres:16-alpine"
    - path: "alembic/env.py"
      provides: "Async Alembic migration environment"
      contains: "run_async_migrations"
  key_links:
    - from: "src/db/postgres.py"
      to: "src/settings.py"
      via: "DATABASE_URL from settings"
      pattern: "settings\\.database_url"
    - from: "alembic/env.py"
      to: "src/db/models.py"
      via: "target_metadata = Base.metadata"
      pattern: "target_metadata"
    - from: "docker-compose.yml"
      to: ".env.example"
      via: "POSTGRES_PASSWORD environment variable"
      pattern: "POSTGRES_PASSWORD"
---

<objective>
Establish the dual-database persistence layer, Docker development environment, and application settings that all subsequent Phase 9 plans depend on.

Purpose: Every v2.0 feature requires PostgreSQL for forecast persistence and Redis for caching. This plan creates the foundation: ORM models for all 5 PostgreSQL tables (predictions, outcome_records, calibration_weights, ingest_runs, api_keys), async connection management, Alembic migrations, Docker Compose for local development, and centralized settings via pydantic-settings.

Output: Running `docker-compose up -d postgres redis` starts the database services. Running `alembic upgrade head` creates all tables. Python code can import `src.db.postgres.get_async_session` and `src.db.models.Prediction` to read/write forecasts.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-api-foundation/09-CONTEXT.md
@.planning/phases/09-api-foundation/09-RESEARCH.md

# Existing SQLAlchemy pattern to follow:
@src/calibration/prediction_store.py

# DTO contract for understanding what data the tables must store:
@WORLDMONITOR_INTEGRATION.md (lines 328-382)

# Current dependencies:
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Dependencies, Settings, and Docker infrastructure</name>
  <files>
    pyproject.toml
    src/settings.py
    docker-compose.yml
    Dockerfile
    .env.example
    .dockerignore
  </files>
  <action>
    1. Update pyproject.toml:
       - Add: fastapi (>=0.115), uvicorn[standard] (>=0.34), asyncpg (>=0.30), sqlalchemy[asyncio] (already has sqlalchemy>=2.0.0 — add asyncio extra), alembic (>=1.14), redis[hiredis] (>=5.0), pydantic-settings (>=2.0), python-multipart (>=0.0.9)
       - Remove: jraph>=0.0.6.dev0 (line 48)
       - Do NOT remove jraph yet from the actual code — Plan 03 handles that atomically. Only remove from pyproject.toml here so `uv sync` stops pulling it.
       - Bump version to "2.0.0-dev"

    2. Create src/settings.py using pydantic-settings BaseSettings:
       - DATABASE_URL: str = "postgresql+asyncpg://geopol:geopol_dev@localhost:5432/geopol"
       - REDIS_URL: str = "redis://localhost:6379/0"
       - GDELT_DB_PATH: str = "data/events.db"
       - ENVIRONMENT: Literal["development", "production", "testing"] = "development"
       - API_KEY_HEADER: str = "X-API-Key"
       - CORS_ORIGINS: list[str] = ["http://localhost:5173", "http://localhost:3000"]
       - LOG_LEVEL: str = "INFO"
       - LOG_JSON: bool = False (True in production)
       - model_config with env_file=".env", env_file_encoding="utf-8"
       - Singleton getter: `_settings = None; def get_settings() -> Settings`

    3. Create docker-compose.yml with three services:
       - postgres: postgres:16-alpine, env POSTGRES_DB/USER/PASSWORD from .env, port 5432, volume pgdata, healthcheck using pg_isready
       - redis: redis:7-alpine, port 6379, healthcheck using redis-cli ping
       - api: build from Dockerfile, port 8000, env vars (DATABASE_URL, REDIS_URL, ENVIRONMENT, GDELT_DB_PATH), volume ./data:/app/data, depends_on postgres+redis with condition: service_healthy
       - Named volume: pgdata

    4. Create Dockerfile (multi-stage not needed yet — single stage):
       - FROM python:3.11-slim
       - Install uv via pip
       - COPY pyproject.toml uv.lock (if exists) .
       - RUN uv sync --no-dev
       - COPY src/ src/
       - COPY alembic/ alembic/
       - COPY alembic.ini .
       - EXPOSE 8000
       - CMD ["uv", "run", "uvicorn", "src.api.app:create_app", "--host", "0.0.0.0", "--port", "8000", "--factory"]

    5. Create .env.example with all settings and safe defaults.

    6. Create .dockerignore (data/, .git/, __pycache__/, .planning/, tests/, .env)

    7. Run `uv sync` to install new dependencies and remove jraph from the lockfile.
  </action>
  <verify>
    - `uv sync` completes without errors
    - `uv run python -c "import fastapi; import asyncpg; import alembic; import redis; print('OK')"` succeeds
    - `uv run python -c "from src.settings import get_settings; s = get_settings(); print(s.DATABASE_URL)"` prints the default URL
    - `docker-compose config` validates without errors (do NOT start containers yet)
  </verify>
  <done>pyproject.toml has all new dependencies and no jraph. Settings load from env. Docker Compose config validates. Dockerfile builds context.</done>
</task>

<task type="auto">
  <name>Task 2: PostgreSQL ORM models, async connection manager, and Alembic migrations</name>
  <files>
    src/db/__init__.py
    src/db/postgres.py
    src/db/sqlite.py
    src/db/models.py
    alembic.ini
    alembic/env.py
    alembic/script.py.mako
    alembic/versions/.gitkeep
  </files>
  <action>
    1. Create src/db/__init__.py (empty or minimal exports).

    2. Create src/db/models.py with SQLAlchemy 2.0 declarative ORM models:
       - Use `class Base(DeclarativeBase): pass` (modern pattern, NOT `declarative_base()`)
       - Use `Mapped[T]` + `mapped_column()` syntax throughout (NOT legacy `Column()`)
       - Models: Prediction, OutcomeRecord, CalibrationWeight, IngestRun, ApiKey
       - Follow the exact schema from 09-RESEARCH.md "PostgreSQL ORM Models" section
       - Use UUID strings (String(36)) for Prediction.id, not Integer — forecasts need stable IDs across systems
       - Use `datetime.now(timezone.utc)` not `datetime.utcnow()` (deprecated)
       - Add composite index on (country_iso, created_at) for Prediction
       - Add proper __repr__ for each model

    3. Create src/db/postgres.py:
       - Import settings from src.settings
       - Create async engine via create_async_engine with pool_size=5, max_overflow=10, pool_timeout=30, pool_recycle=1800
       - Create async_session_factory via async_sessionmaker
       - Implement get_async_session() as async generator with commit/rollback/close
       - Implement init_db() that creates engine from settings
       - Implement close_db() for graceful shutdown

    4. Create src/db/sqlite.py:
       - SQLiteConnection class with context manager
       - WAL mode, busy_timeout=30000, synchronous=NORMAL
       - Uses GDELT_DB_PATH from settings
       - This wraps the existing sqlite3 pattern — the existing src/database/ module stays as-is for now

    5. Initialize Alembic with async template:
       - Run `alembic init -t async alembic` (or manually create the files)
       - Place alembic.ini at project root
       - Configure env.py to:
         a. Import Base from src.db.models
         b. Import settings for DATABASE_URL
         c. Set target_metadata = Base.metadata
         d. Use async engine via run_async_migrations pattern
       - Generate initial migration: `alembic revision --autogenerate -m "initial schema"`

    6. Verify migration applies cleanly against a running PostgreSQL:
       - Start postgres via docker-compose: `docker-compose up -d postgres`
       - Run `alembic upgrade head`
       - Verify tables exist
  </action>
  <verify>
    - `docker-compose up -d postgres redis` starts services (check with `docker-compose ps`)
    - `uv run alembic upgrade head` creates all 5 tables without errors
    - `uv run python -c "from src.db.models import Prediction, OutcomeRecord, CalibrationWeight, IngestRun, ApiKey; print('Models OK')"` succeeds
    - `uv run python -c "
import asyncio
from src.db.postgres import get_async_session, engine
from src.db.models import ApiKey
from sqlalchemy import select, text

async def test():
    async for session in get_async_session():
        result = await session.execute(text('SELECT 1'))
        print('Connection OK:', result.scalar())
        break

asyncio.run(test())
"` prints "Connection OK: 1"
    - `uv run python -c "from src.db.sqlite import SQLiteConnection; c = SQLiteConnection(); print('SQLite OK')"` succeeds
  </verify>
  <done>All 5 PostgreSQL tables exist and are accessible via async SQLAlchemy sessions. Alembic migration is version-controlled. SQLite connection manager provides WAL-mode access for GDELT store.</done>
</task>

</tasks>

<verification>
1. `docker-compose up -d postgres redis` — both services healthy within 15 seconds
2. `uv run alembic upgrade head` — applies cleanly, all 5 tables created
3. Python can import all models and connect to PostgreSQL asynchronously
4. SQLite connection manager works with WAL mode
5. Settings load from environment with sensible defaults
</verification>

<success_criteria>
- PostgreSQL running via Docker with predictions, outcome_records, calibration_weights, ingest_runs, api_keys tables
- Redis running via Docker
- Async SQLAlchemy sessions connect to PostgreSQL with connection pooling
- SQLite WAL-mode connection manager exists for GDELT store
- Alembic migration version-controlled at project root
- All new dependencies installed, jraph removed from pyproject.toml
- Settings centralized via pydantic-settings with env file support
</success_criteria>

<output>
After completion, create `.planning/phases/09-api-foundation/09-01-SUMMARY.md`
</output>
