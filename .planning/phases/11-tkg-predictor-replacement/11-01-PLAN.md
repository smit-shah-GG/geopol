---
phase: 11-tkg-predictor-replacement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/training/models/components/__init__.py
  - src/training/models/components/global_history.py
  - src/training/models/components/time_conv_transe.py
  - src/training/models/tirgn_jax.py
  - tests/test_tirgn_model.py
autonomous: true

must_haves:
  truths:
    - "TiRGN nnx.Module forward pass produces entity embeddings of shape (num_entities, embedding_dim) from a sequence of GraphSnapshots"
    - "Global history encoder builds sparse binary vocabulary matrix per-timestamp and produces history-constrained entity scores"
    - "Time-ConvTransE decoder scores triples using 4-channel input (subject, relation, time_periodic, time_nonperiodic) through 1D convolution"
    - "Copy-generation fusion interpolates raw and history-based distributions via sigmoid-gated history_rate parameter"
    - "compute_loss() returns NLL loss over fused distribution (not margin ranking loss)"
    - "TiRGN satisfies TKGModelProtocol (isinstance check passes at runtime)"
    - "Mixed precision via dtype=jnp.bfloat16 on forward pass, param_dtype=jnp.float32 for master weights"
  artifacts:
    - path: "src/training/models/components/global_history.py"
      provides: "GlobalHistoryEncoder -- sparse binary vocabulary matrix construction and history-constrained decoding"
      min_lines: 80
    - path: "src/training/models/components/time_conv_transe.py"
      provides: "TimeConvTransEDecoder -- 4-channel ConvTransE with learned periodic and non-periodic time embeddings"
      min_lines: 60
    - path: "src/training/models/tirgn_jax.py"
      provides: "TiRGN nnx.Module -- full model composing R-GCN local encoder, GRU temporal evolution, global history encoder, Time-ConvTransE decoder, and copy-generation fusion"
      min_lines: 250
    - path: "tests/test_tirgn_model.py"
      provides: "Unit tests for TiRGN forward pass, loss computation, protocol compliance, and component correctness"
      min_lines: 100
  key_links:
    - from: "src/training/models/tirgn_jax.py"
      to: "src/protocols/tkg.py"
      via: "TiRGN satisfies TKGModelProtocol structural check"
      pattern: "TKGModelProtocol"
    - from: "src/training/models/tirgn_jax.py"
      to: "src/training/models/components/global_history.py"
      via: "TiRGN.__init__ instantiates GlobalHistoryEncoder"
      pattern: "GlobalHistoryEncoder"
    - from: "src/training/models/tirgn_jax.py"
      to: "src/training/models/components/time_conv_transe.py"
      via: "TiRGN.__init__ instantiates TimeConvTransEDecoder (raw + history)"
      pattern: "TimeConvTransEDecoder"
    - from: "src/training/models/tirgn_jax.py"
      to: "src/training/models/regcn_jax.py"
      via: "Imports GraphSnapshot NamedTuple (shared data format)"
      pattern: "from src.training.models.regcn_jax import GraphSnapshot"
---

<objective>
Implement the TiRGN model as a Flax NNX module in JAX, porting the full architecture from the reference PyTorch/DGL implementation (Li et al., IJCAI 2022). This includes the global history encoder, Time-ConvTransE decoder, and copy-generation fusion mechanism -- the three components that differentiate TiRGN from our existing RE-GCN.

Purpose: TiRGN's global history encoder captures repeated facts across all prior timestamps, addressing RE-GCN's blindness to long-range temporal patterns. The published improvement on GDELT is +10.1% MRR. This plan delivers the model architecture; training and integration follow in Plans 02 and 03.

Output: `TiRGN` nnx.Module class with all subcomponents, unit tests verifying forward pass shapes and protocol compliance, mixed precision support for RTX 3060 VRAM efficiency.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-tkg-predictor-replacement/11-CONTEXT.md
@.planning/phases/11-tkg-predictor-replacement/11-RESEARCH.md
@src/training/models/regcn_jax.py
@src/protocols/tkg.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Global history encoder and Time-ConvTransE decoder components</name>
  <files>
    src/training/models/components/__init__.py
    src/training/models/components/global_history.py
    src/training/models/components/time_conv_transe.py
  </files>
  <action>
  Create `src/training/models/components/` package with two new modules.

  **src/training/models/components/__init__.py**: Exports `GlobalHistoryEncoder`, `TimeConvTransEDecoder`.

  **src/training/models/components/global_history.py** -- `GlobalHistoryEncoder`:

  This is the key TiRGN innovation. It builds a sparse binary "vocabulary" matrix tracking which (subject, relation) pairs have historically produced which objects, then uses this to constrain predictions.

  1. `build_history_vocabulary(snapshots: list[jnp.ndarray], num_entities: int, num_relations: int, window_size: int = 50) -> jnp.ndarray`:
     - Static function (not an nnx.Module method -- called once during data preprocessing, not during training forward pass).
     - Input: list of snapshot triple arrays, each shape (num_triples, 3) with columns [subject, relation, object].
     - For each (subject, relation) pair seen in the last `window_size` snapshots, set a 1-bit for each object that appeared with that pair.
     - Output shape: (num_entities * num_relations, num_entities) -- boolean/uint8 sparse representation.
     - **CRITICAL**: Do NOT materialize a dense (E*R, E) matrix for large entity counts. Use a dictionary-of-keys representation internally, convert to a JAX-compatible format per-batch. For GDELT scale (~500K entities, ~300 relations), the dense matrix would be ~28GB. Instead, return a dictionary mapping `(subject, relation) -> set[object]` and provide a `get_history_mask(subjects: Array, relations: Array, num_entities: int) -> Array` function that constructs per-batch boolean masks of shape (batch_size, num_entities).
     - The window_size parameter controls how many past timestamps to consider. Default 50 (per CONTEXT.md).

  2. `GlobalHistoryEncoder(nnx.Module)`:
     - Constructor: `__init__(self, num_entities, num_relations, embedding_dim, *, rngs)`.
     - Owns a `TimeConvTransEDecoder` instance (the "history decoder" -- separate from the raw decoder).
     - `__call__(self, entity_emb, relation_emb, history_mask, time_emb_periodic, time_emb_nonperiodic) -> Array`:
       - Scores all entities using the history decoder: `hist_scores = self.hist_decoder(entity_emb, triples_for_all_entities, time_emb_periodic, time_emb_nonperiodic)`.
       - Applies history_mask: set scores for entities NOT in history to -inf BEFORE softmax.
       - Returns softmax probabilities over entities: shape (batch_size, num_entities).

  **src/training/models/components/time_conv_transe.py** -- `TimeConvTransEDecoder`:

  Extends the ConvTransE scoring pattern with temporal awareness.

  1. `TimeConvTransEDecoder(nnx.Module)`:
     - Constructor: `__init__(self, embedding_dim, num_relations, num_filters=32, kernel_size=3, dropout_rate=0.2, *, rngs)`.
     - Owns: relation embeddings (num_relations, embedding_dim), conv weights, batch norm, FC projection, dropout.
     - Learned time embeddings: `time_periodic_emb` (max_time_steps, embedding_dim) and `time_nonperiodic_emb` (max_time_steps, embedding_dim). Use `max_time_steps=366` (one year of daily snapshots).
     - `__call__(self, entity_emb, triples, time_indices, training=True) -> Array`:
       - Input: entity_emb (num_entities, dim), triples (batch, 3), time_indices (batch,) -- integer day index into time embeddings.
       - Gather subject, relation, time_periodic, time_nonperiodic embeddings.
       - Stack into 4-channel input: (batch, 4*dim, 1).
       - Apply 1D convolution (num_filters kernels of size kernel_size).
       - Apply `nnx.BatchNorm` (feature_axis=-2 for the filter dimension).
       - Apply ReLU activation.
       - Apply dropout (only when training=True).
       - Flatten and project via FC to (batch, dim).
       - Score via dot product with ALL entity embeddings -> (batch, num_entities).
       - Return raw logits (softmax applied later by caller).

  **Implementation notes from RESEARCH.md:**
  - Mixed precision: use `dtype=jnp.bfloat16` for computation tensors, `param_dtype=jnp.float32` for parameters where Flax NNX supports it. For manual parameters (nnx.Param), store in float32 and cast to bfloat16 during forward pass.
  - The history decoder and raw decoder are separate `TimeConvTransEDecoder` instances with independent weights.
  - `nnx.BatchNorm` in Flax NNX: use `nnx.BatchNorm(num_features=num_filters, rngs=rngs)`. Call with `use_running_average=not training`.
  </action>
  <verify>
  `python -c "from src.training.models.components import GlobalHistoryEncoder, TimeConvTransEDecoder; print('imports OK')"` succeeds.
  `python -c "
from src.training.models.components.global_history import build_history_vocabulary
import numpy as np
snaps = [np.array([[0,0,1],[1,1,2]]), np.array([[0,0,2],[2,0,1]])]
vocab = build_history_vocabulary(snaps, 3, 2, window_size=50)
# Check that (entity=0, rel=0) has history of objects {1, 2}
mask = vocab[(0, 0)]
assert 1 in mask and 2 in mask, f'Expected {{1,2}}, got {mask}'
print('history vocab OK')
"` succeeds.
  </verify>
  <done>
  GlobalHistoryEncoder and TimeConvTransEDecoder are implemented as Flax NNX modules with correct shapes, mixed precision support, and sparse history vocabulary construction that avoids 28GB dense matrix.
  </done>
</task>

<task type="auto">
  <name>Task 2: TiRGN model module with protocol compliance and unit tests</name>
  <files>
    src/training/models/tirgn_jax.py
    tests/test_tirgn_model.py
  </files>
  <action>
  Build the top-level TiRGN module composing all components, and comprehensive unit tests.

  **src/training/models/tirgn_jax.py** -- `TiRGN(nnx.Module)`:

  Port of the full TiRGN architecture (Li et al., IJCAI 2022). This is a clean Flax NNX implementation, NOT a wrapper around the existing REGCN class.

  1. Constructor `__init__(self, num_entities, num_relations, embedding_dim=200, num_layers=2, num_bases=30, dropout_rate=0.2, history_rate=0.3, history_window=50, *, rngs)`:
     - `self.num_entities`, `self.num_relations`, `self.embedding_dim` -- exposed as attributes (required by TKGModelProtocol).
     - `self.entity_emb = nnx.Param(...)` -- initial entity embeddings (num_entities, embedding_dim), xavier init.
     - `self.rgcn_layers = nnx.List([RelationalGraphConv(...) for _ in range(num_layers)])` -- reuse the existing RelationalGraphConv from regcn_jax.py (import it, do NOT rewrite). The R-GCN layer is identical between RE-GCN and TiRGN.
     - `self.entity_gru = GRUCell(embedding_dim, rngs=rngs)` -- reuse existing GRUCell from regcn_jax.py.
     - `self.relation_gru` -- **NEW**: GRU for relation embedding evolution. This is TiRGN-specific. The existing GRUCell assumes input_dim == hidden_dim. For relation evolution, input comes from aggregated edge features (dimension may differ from embedding_dim). **Mandated approach**: add `self.rel_input_proj = nnx.Linear(in_features=embedding_dim, out_features=embedding_dim, rngs=rngs)` to project aggregated relation features before the existing GRUCell. This avoids modifying the shared GRUCell class and matches the R-GCN aggregation output dimension. Do NOT create a modified GRUCell -- use the projection layer + existing GRUCell.
     - `self.raw_decoder = TimeConvTransEDecoder(embedding_dim, num_relations * 2, rngs=rngs)` -- scores using evolved entity embeddings.
     - `self.global_history = GlobalHistoryEncoder(num_entities, num_relations * 2, embedding_dim, rngs=rngs)` -- scores using history-constrained distribution.
     - `self.history_rate: float = history_rate` -- scalar alpha for copy-generation fusion. NOT a learned parameter (configurable hyperparameter per CONTEXT.md).
     - `self.history_window: int = history_window`.

  2. `_encode_snapshot(self, x, snapshot, training=True) -> Array`:
     - Same pattern as REGCN.encode_snapshot: pass through R-GCN layers with ReLU + dropout.
     - Returns updated entity embeddings.

  3. `evolve_embeddings(self, snapshots: list, training=False, **kwargs) -> Array`:
     - Process temporal snapshots through R-GCN + entity GRU (same as RE-GCN temporal loop).
     - Use `jax.checkpoint` on each snapshot for memory efficiency.
     - Also evolve relation embeddings through relation GRU (TiRGN-specific): aggregate per-relation edge features per snapshot, project, GRU step.
     - Return final entity embeddings of shape (num_entities, embedding_dim).
     - **Signature matches TKGModelProtocol.evolve_embeddings**.

  4. `compute_scores(self, entity_emb: Array, triples: Array) -> Array`:
     - Score triples using the raw decoder only (no history fusion during scoring).
     - Return shape (batch,).
     - **Signature matches TKGModelProtocol.compute_scores**.

  5. `compute_loss(self, snapshots: list, pos_triples: Array, neg_triples: Array, margin: float = 1.0, **kwargs) -> Array`:
     - Evolve embeddings through snapshots.
     - Compute raw scores via raw_decoder for all entities (not just pos/neg): shape (batch, num_entities).
     - Compute history scores via global_history for all entities: shape (batch, num_entities).
     - **Copy-generation fusion**: `fused_logits = history_rate * hist_probs + (1 - history_rate) * raw_probs` where probs are softmax of the respective scores.
     - **NLL loss**: `-log(fused_probs[target_entity])` averaged over batch. Extract target from pos_triples[:, 2] (object column).
     - The `neg_triples` and `margin` parameters are accepted but ignored (protocol compatibility with RE-GCN's margin-based loss). Log a debug message noting this.
     - **Signature matches TKGModelProtocol.compute_loss**.

  6. `predict(self, snapshots: list, query_triples: Array) -> Array`:
     - Convenience method (not part of protocol, but matches REGCN pattern).
     - Evolve embeddings, compute fused scores, return.

  7. Factory function `create_tirgn_model(num_entities, num_relations, embedding_dim=200, num_layers=2, history_rate=0.3, history_window=50, seed=0) -> TiRGN`.

  **CRITICAL design decisions:**
  - Import `RelationalGraphConv`, `GRUCell`, `GraphSnapshot` from `src.training.models.regcn_jax` -- reuse existing components, do NOT duplicate.
  - Import `GlobalHistoryEncoder` from `src.training.models.components.global_history`.
  - Import `TimeConvTransEDecoder` from `src.training.models.components.time_conv_transe`.
  - The history vocabulary dict is NOT stored inside the model. It's passed as an argument to compute_loss() via **kwargs (key: `history_vocab`). The training loop is responsible for building and passing it.

  **tests/test_tirgn_model.py**:

  1. `test_tirgn_forward_shapes`: Create small TiRGN (10 entities, 5 relations, dim=32). Generate 3 random GraphSnapshots. Call evolve_embeddings(), verify output shape (10, 32).
  2. `test_tirgn_compute_scores_shape`: After evolve_embeddings, call compute_scores with 5 random triples. Verify output shape (5,).
  3. `test_tirgn_compute_loss_scalar`: Call compute_loss with snapshots and pos/neg triples. Verify output is scalar (shape ()). Verify it's finite and non-negative.
  4. `test_tirgn_protocol_compliance`: `from src.protocols.tkg import TKGModelProtocol; assert isinstance(model, TKGModelProtocol)`. Verify all three protocol methods exist with correct signatures.
  5. `test_tirgn_attributes`: Verify `model.num_entities`, `model.num_relations`, `model.embedding_dim` attributes exist and have correct values.
  6. `test_time_conv_transe_shape`: Standalone test of TimeConvTransEDecoder with known input shapes.
  7. `test_global_history_mask`: Test that history mask correctly zeroes out non-history entities in the score distribution.
  8. `test_copy_generation_fusion`: Test that fused distribution is a valid probability distribution (sums to ~1, all values >= 0).
  9. `test_mixed_precision`: Verify that intermediate computations use bfloat16 when model is configured for it.
  </action>
  <verify>
  `python -c "from src.training.models.tirgn_jax import TiRGN, create_tirgn_model; print('import OK')"` succeeds.
  `python -c "
from src.protocols.tkg import TKGModelProtocol
from src.training.models.tirgn_jax import create_tirgn_model
m = create_tirgn_model(100, 10, embedding_dim=64)
assert isinstance(m, TKGModelProtocol), 'TiRGN does not satisfy TKGModelProtocol'
print('protocol check passed')
"` succeeds.
  `uv run pytest tests/test_tirgn_model.py -v` -- all tests pass.
  </verify>
  <done>
  TiRGN model module is complete with all components wired together, satisfies TKGModelProtocol, produces correct output shapes, and computes NLL loss with copy-generation fusion. 9 unit tests pass covering forward pass, loss, protocol compliance, component correctness, and mixed precision.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.training.models.components import GlobalHistoryEncoder, TimeConvTransEDecoder"` -- imports succeed.
2. `python -c "from src.training.models.tirgn_jax import TiRGN, create_tirgn_model"` -- imports succeed.
3. `python -c "from src.protocols.tkg import TKGModelProtocol; from src.training.models.tirgn_jax import create_tirgn_model; m = create_tirgn_model(100, 10, embedding_dim=64); assert isinstance(m, TKGModelProtocol)"` -- protocol check passes.
4. `uv run pytest tests/test_tirgn_model.py -v` -- all 9 tests pass.
5. No modifications to existing regcn_jax.py (only imports from it).
</verification>

<success_criteria>
- TiRGN nnx.Module is fully implemented with global history encoder, Time-ConvTransE decoder, and copy-generation fusion.
- Model satisfies TKGModelProtocol at runtime (isinstance check).
- Sparse history vocabulary avoids 28GB dense matrix by using dictionary-of-keys representation.
- Mixed precision support (bfloat16 compute, float32 params) is built into the architecture.
- Unit tests verify shapes, loss computation, protocol compliance, and component correctness.
- Requirements TKG-01 and TKG-02 are covered.
</success_criteria>

<output>
After completion, create `.planning/phases/11-tkg-predictor-replacement/11-01-SUMMARY.md`
</output>
