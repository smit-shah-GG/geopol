---
phase: 11-tkg-predictor-replacement
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - pyproject.toml
  - src/training/training_logger.py
  - src/training/train_tirgn.py
  - scripts/train_tirgn.py
  - tests/test_train_tirgn.py
autonomous: true
user_setup:
  - service: wandb
    why: "Optional cloud experiment tracking for TiRGN training runs"
    env_vars:
      - name: WANDB_API_KEY
        source: "wandb.ai → Settings → API keys (optional -- TensorBoard works without it)"

must_haves:
  truths:
    - "Running scripts/train_tirgn.py trains TiRGN to completion on GDELT data, logging per-epoch MRR/Hits@K/loss to TensorBoard"
    - "Training uses mixed precision (bfloat16 forward, float32 params) and gradient checkpointing by default"
    - "Early stopping on validation MRR halts training when no improvement for configurable patience epochs"
    - "Best model checkpoint is saved as .npz + .json metadata with entity/relation mappings"
    - "VRAM usage is logged per-epoch via jax.devices()[0].memory_stats()"
    - "When WANDB_API_KEY is set, metrics are also logged to Weights & Biases (no crash when unset)"
    - "Global history vocabulary is pre-computed once before training and passed to compute_loss() per batch"
  artifacts:
    - path: "src/training/training_logger.py"
      provides: "TrainingLogger abstraction -- TensorBoard always-on, W&B optional"
      min_lines: 80
    - path: "src/training/train_tirgn.py"
      provides: "TiRGN training loop with data loading, negative sampling, evaluation, checkpointing, and early stopping"
      min_lines: 200
    - path: "scripts/train_tirgn.py"
      provides: "CLI entry point for TiRGN training with argparse (--epochs, --lr, --batch-size, --history-rate, --history-window, --patience, --logdir)"
      min_lines: 50
    - path: "tests/test_train_tirgn.py"
      provides: "Unit tests for training loop components (data loading, history vocab construction, early stopping logic, logger)"
      min_lines: 60
  key_links:
    - from: "src/training/train_tirgn.py"
      to: "src/training/models/tirgn_jax.py"
      via: "Imports create_tirgn_model, calls model.compute_loss() in training loop"
      pattern: "create_tirgn_model"
    - from: "src/training/train_tirgn.py"
      to: "src/training/training_logger.py"
      via: "TrainingLogger.log_metrics() called per-epoch with loss, MRR, VRAM"
      pattern: "TrainingLogger"
    - from: "src/training/train_tirgn.py"
      to: "src/training/train_jax.py"
      via: "Reuses load_gdelt_data(), create_graph_snapshots(), negative_sampling(), compute_mrr() from existing RE-GCN training"
      pattern: "from src.training.train_jax import"
    - from: "scripts/train_tirgn.py"
      to: "src/training/train_tirgn.py"
      via: "Calls train_tirgn() with CLI-parsed config"
      pattern: "train_tirgn"
---

<objective>
Build the TiRGN training pipeline with observability, reusing the existing GDELT data loading and evaluation infrastructure from the RE-GCN training loop. Add a training logger abstraction that writes to TensorBoard always and Weights & Biases when configured.

Purpose: Without a training loop, the TiRGN model from Plan 01 is just architecture. This plan makes it trainable and observable, enabling the MRR comparison (TKG-03) and GPU envelope validation (TKG-04) that determine whether TiRGN ships.

Output: Runnable `scripts/train_tirgn.py` that trains TiRGN on GDELT data with full observability (TensorBoard + optional W&B), early stopping, mixed precision, gradient checkpointing, and VRAM monitoring.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-tkg-predictor-replacement/11-CONTEXT.md
@.planning/phases/11-tkg-predictor-replacement/11-RESEARCH.md
@.planning/phases/11-tkg-predictor-replacement/11-01-SUMMARY.md
@src/training/train_jax.py
@src/training/models/tirgn_jax.py
@src/training/models/regcn_jax.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Dependencies and TrainingLogger abstraction</name>
  <files>
    pyproject.toml
    src/training/training_logger.py
  </files>
  <action>
  1. Add dependencies to pyproject.toml: `tensorboardX>=2.6`, `wandb>=0.16` (as optional). Run `uv sync`.

  **src/training/training_logger.py** -- `TrainingLogger`:

  Unified logging abstraction that writes to TensorBoard (always) and W&B (when API key is available). Designed for both manual training runs and automated retraining via the scheduler.

  1. `TrainingLogger` class:
     - Constructor: `__init__(self, logdir: str | Path, project: str = "geopol-tkg", run_name: str | None = None, config: dict | None = None)`.
     - On init:
       a. Create `tensorboardX.SummaryWriter(logdir)`. This is always available.
       b. Check `os.environ.get("WANDB_API_KEY")`. If present, call `wandb.init(project=project, name=run_name, config=config)`. If not present or if import fails, log info "W&B not configured, TensorBoard only" and set `self._wandb_run = None`.
     - `log_metrics(self, metrics: dict[str, float], step: int) -> None`:
       - Write each metric to TensorBoard via `self.writer.add_scalar(key, value, step)`.
       - If W&B active: `wandb.log(metrics, step=step)`.
     - `log_text(self, tag: str, text: str, step: int) -> None`:
       - TensorBoard: `self.writer.add_text(tag, text, step)`.
     - `close(self) -> None`:
       - `self.writer.close()`.
       - If W&B active: `wandb.finish()`.
     - Context manager support: `__enter__` returns self, `__exit__` calls close().

  2. Log metric naming convention (for consistency across runs):
     - `train/loss` -- per-epoch training loss
     - `train/lr` -- learning rate
     - `eval/mrr` -- validation MRR
     - `eval/hits_at_1`, `eval/hits_at_3`, `eval/hits_at_10`
     - `system/vram_used_mb` -- GPU memory usage
     - `system/epoch_duration_s` -- wall-clock time per epoch
     - `system/total_params` -- logged once at step 0

  **Implementation notes:**
  - tensorboardX is preferred over torch.utils.tensorboard because it has no PyTorch dependency. It's a pure-Python TensorBoard writer.
  - wandb is truly optional -- the logger must never crash if wandb is not installed or not configured.
  - Use try/except around all wandb calls to handle ImportError gracefully.
  </action>
  <verify>
  `uv sync` succeeds.
  `python -c "from src.training.training_logger import TrainingLogger; print('import OK')"` succeeds.
  `python -c "
import tempfile, os
os.environ.pop('WANDB_API_KEY', None)  # Ensure W&B is disabled
from src.training.training_logger import TrainingLogger
with TrainingLogger(tempfile.mkdtemp(), run_name='test') as logger:
    logger.log_metrics({'train/loss': 0.5, 'eval/mrr': 0.15}, step=1)
    print('logging OK')
"` succeeds without errors.
  </verify>
  <done>
  tensorboardX and wandb dependencies installed. TrainingLogger writes metrics to TensorBoard always, W&B when configured. Logger gracefully degrades when W&B is unavailable.
  </done>
</task>

<task type="auto">
  <name>Task 2: TiRGN training loop with early stopping and VRAM monitoring</name>
  <files>
    src/training/train_tirgn.py
    scripts/train_tirgn.py
    tests/test_train_tirgn.py
  </files>
  <action>
  Build the TiRGN-specific training loop, reusing data infrastructure from train_jax.py.

  **src/training/train_tirgn.py** -- `train_tirgn()`:

  1. `TiRGNTrainingConfig(dataclass)`:
     - Inherits/extends TrainingConfig fields: epochs=100, learning_rate=0.001, batch_size=1024, num_negatives=10, grad_clip=1.0, checkpoint_interval=10, eval_interval=5.
     - TiRGN-specific: history_rate=0.3, history_window=50, patience=15 (early stopping patience), logdir="runs/tirgn".

  2. `build_history_vocabulary_from_snapshots(snapshots_np: list[np.ndarray], num_entities: int, num_relations: int, window_size: int) -> dict`:
     - Wraps `build_history_vocabulary()` from `src.training.models.components.global_history`.
     - Called ONCE before training begins.
     - Logs vocabulary statistics: total (s,r) pairs with history, average objects per pair.

  3. `train_tirgn(data_path, config, model_dir, max_events=0, num_days=30, embedding_dim=200, num_layers=2) -> dict`:
     - **Data loading**: Reuse `load_gdelt_data()` and `create_graph_snapshots()` from `src.training.train_jax`. Do NOT reimplement data loading.
     - **Model creation**: `create_tirgn_model(num_entities, num_relations, embedding_dim, num_layers, config.history_rate, config.history_window, seed=0)`.
     - **History vocabulary**: Call `build_history_vocabulary_from_snapshots()` with the training snapshots.
     - **Optimizer**: `optax.chain(optax.clip_by_global_norm(config.grad_clip), optax.adam(config.learning_rate))` wrapped in `nnx.Optimizer(model, tx, wrt=nnx.Param)`.
     - **Logger**: `TrainingLogger(config.logdir, run_name=f"tirgn_{datetime.now():%Y%m%d_%H%M}", config=asdict(config))`.
     - Log total params at step 0.

     **Training loop** (per-epoch):
     a. Shuffle training data.
     b. For each batch:
        - Generate negative samples via `negative_sampling()` (reuse from train_jax.py).
        - Compute loss: `nnx.value_and_grad(loss_fn)(model)` where `loss_fn` calls `model.compute_loss(snapshots, pos_jax, neg_jax, history_vocab=history_vocab)`.
        - Update via optimizer.
     c. Log VRAM: `jax.devices()[0].memory_stats()` -- extract `peak_bytes_in_use`, convert to MB, log as `system/vram_used_mb`. Wrap in try/except (memory_stats() may not be available on all backends).
     d. Log epoch duration.
     e. Evaluate every `eval_interval` epochs:
        - Compute MRR/Hits@K via `compute_mrr()` from train_jax.py (reuse).
        - Log all metrics via TrainingLogger.
        - **Early stopping check**: if MRR hasn't improved for `patience` epochs, stop training. Log "Early stopping at epoch {N}, best MRR {M:.4f} at epoch {K}".
        - Save best model checkpoint when new best MRR achieved.
     f. Save periodic checkpoints every `checkpoint_interval` epochs.

     **Checkpoint format**: Same as RE-GCN (save_checkpoint from train_jax.py), with additional metadata:
     - `model_type: "tirgn"` in the JSON metadata.
     - `history_rate`, `history_window` in config section.

     **Return**: `{"status": "complete", "epochs_trained": N, "best_mrr": float, "total_time": float, "early_stopped": bool, "model_type": "tirgn"}`.

  **scripts/train_tirgn.py** -- CLI entry point:

  1. argparse with:
     - `--data-path` (default: data/gdelt/processed/events.parquet)
     - `--model-dir` (default: models/tkg)
     - `--epochs` (default: 100)
     - `--lr` (default: 0.001)
     - `--batch-size` (default: 1024)
     - `--history-rate` (default: 0.3)
     - `--history-window` (default: 50)
     - `--patience` (default: 15)
     - `--logdir` (default: runs/tirgn)
     - `--max-events` (default: 0)
     - `--num-days` (default: 30)
     - `--embedding-dim` (default: 200)
     - `--num-layers` (default: 2)
  2. Initialize logging.
  3. Call `train_tirgn()` with parsed args.
  4. Print final metrics summary.

  **tests/test_train_tirgn.py**:

  1. `test_build_history_vocabulary`: Small synthetic data, verify vocabulary dict has expected entries.
  2. `test_early_stopping_triggers`: Mock training loop internals, verify early stopping fires after patience epochs with no improvement.
  3. `test_training_config_defaults`: Verify TiRGNTrainingConfig has correct default values for history_rate, history_window, patience.
  4. `test_checkpoint_metadata_includes_model_type`: Create a checkpoint, verify JSON metadata contains `model_type: "tirgn"`.
  5. `test_logger_metrics_format`: Verify TrainingLogger logs expected metric keys.

  Do NOT run actual GPU training in tests. Mock JAX operations and model forward passes where needed.
  </action>
  <verify>
  `python -c "from src.training.train_tirgn import train_tirgn, TiRGNTrainingConfig; print('import OK')"` succeeds.
  `python scripts/train_tirgn.py --help` shows all CLI arguments.
  `uv run pytest tests/test_train_tirgn.py -v` -- all tests pass.
  </verify>
  <done>
  TiRGN training loop is runnable with early stopping, mixed precision, VRAM monitoring, TensorBoard + optional W&B logging. CLI entry point accepts all hyperparameters. Unit tests verify training infrastructure without GPU dependency. Requirements TKG-03 (evaluation infrastructure) and TKG-04 (GPU envelope with mixed precision + gradient checkpointing) are structurally covered.
  </done>
</task>

</tasks>

<verification>
1. `uv sync` installs tensorboardX and wandb.
2. `python -c "from src.training.training_logger import TrainingLogger"` -- import succeeds.
3. `python -c "from src.training.train_tirgn import train_tirgn, TiRGNTrainingConfig"` -- import succeeds.
4. `python scripts/train_tirgn.py --help` -- shows CLI usage with all expected arguments.
5. `uv run pytest tests/test_train_tirgn.py -v` -- all tests pass.
6. TensorBoard logdir structure is created when logger is instantiated.
7. No modifications to existing train_jax.py (only imports from it).
</verification>

<success_criteria>
- TiRGN training loop reuses existing data loading and evaluation functions from train_jax.py (no duplication).
- TrainingLogger writes to TensorBoard always, W&B when configured, never crashes on missing W&B.
- Early stopping halts training when validation MRR plateaus.
- VRAM usage is logged per-epoch for GPU envelope validation.
- Checkpoint format includes model_type discriminator for downstream model loading.
- CLI entry point exposes all hyperparameters including TiRGN-specific history_rate and history_window.
- All unit tests pass without GPU.
</success_criteria>

<output>
After completion, create `.planning/phases/11-tkg-predictor-replacement/11-02-SUMMARY.md`
</output>
