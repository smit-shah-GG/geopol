# Phase 2 Plan 1: TKG Construction from GDELT Events

## Goal
Build temporal knowledge graph from GDELT event stream with entity and relation extraction

## Context
- Phase 1 complete: SQLite database with deduplicated GDELT events
- Focus on conflicts (QuadClass 4) and diplomatic events (QuadClass 1)
- Need CPU-efficient graph structure for ~100K-1M facts

## Approach
Transform raw GDELT events into temporal knowledge graph using NetworkX with structured entity/relation extraction

## Plan

### Task 1: Entity Normalization Module
**Goal**: Map CAMEO actor codes to canonical entity identifiers

**Steps**:
1. Create actor mapping cache for CAMEO codes → canonical IDs
2. Implement entity resolution for country codes, organization codes, and ethnic/religious groups
3. Add fallback for unknown actors with consistent ID generation
4. Test with sample of 1000 events to verify coverage

**Verify**:
- All GDELT actor codes resolve to valid entities
- Cache lookup performance < 1ms per entity
- Unknown actors get consistent IDs across runs

### Task 2: Relation Classification System
**Goal**: Convert CAMEO event codes and QuadClass to typed relations

**Steps**:
1. Create mapping table: (event_code, quad_class) → relation_type
2. Define confidence calculation based on NumMentions, GoldsteinScale, and Tone
3. Implement Bayesian confidence aggregation for duplicate events
4. Add temporal granularity detection (daily for conflicts, weekly for diplomatic)

**Verify**:
- All QuadClass 1 and 4 events map to valid relation types
- Confidence scores range [0,1] with proper calibration
- Aggregation handles duplicate events correctly

### Task 3: NetworkX Graph Builder
**Goal**: Construct temporal MultiDiGraph from normalized triples

**Steps**:
1. Initialize NetworkX MultiDiGraph with temporal edge attributes
2. Stream events from SQLite in 1000-event batches
3. Add nodes with entity metadata (type, name, actor_code)
4. Add edges with temporal and confidence attributes
5. Implement time-window based edge filtering

**Verify**:
- Graph contains expected ~2K unique actors from 100K events
- Edge attributes include timestamp, confidence, quad_class
- Memory usage < 300MB for 1M facts

### Task 4: Temporal Index Creation
**Goal**: Build efficient temporal query structures

**Steps**:
1. Create timestamp-sorted edge index for time-range queries
2. Add actor-pair index for bilateral relation queries
3. Implement temporal neighbor iteration with time constraints
4. Build QuadClass-specific subgraph views

**Verify**:
- Time-range queries return in < 10ms for 30-day windows
- Actor-pair lookups are O(1)
- Subgraph views correctly filter by QuadClass

### Task 5: Graph Persistence Layer
**Goal**: Save and load graph state efficiently

**Steps**:
1. Implement graph serialization to GraphML format
2. Add metadata preservation for nodes and edges
3. Create incremental update mechanism for new events
4. Test round-trip save/load with validation

**Verify**:
- Serialized graph preserves all attributes
- Load time < 5 seconds for 1M-edge graph
- Incremental updates maintain graph consistency

### Task 6: Testing and Benchmarking
**Goal**: Validate correctness and performance

**Steps**:
1. Unit tests for entity normalization and relation classification
2. Integration test: 10K events → graph with validation
3. Performance benchmark: measure throughput for each stage
4. Memory profiling to verify < 300MB for 1M facts

**Verify**:
- 100% test coverage for core modules
- End-to-end pipeline processes 1000 events/second
- Memory usage stays within bounds

## Success Criteria
- [ ] Graph construction from 100K GDELT events completes in < 2 minutes
- [ ] Memory usage < 300MB for 1M facts
- [ ] All QuadClass 1 and 4 events properly classified
- [ ] Temporal queries return in < 10ms
- [ ] Graph serialization/deserialization works correctly

## Time Estimate
3-4 hours

## Dependencies
- SQLite database from Phase 1
- NetworkX library
- Python standard library (no additional ML dependencies)

## Risk Factors
- Actor code coverage might be incomplete (mitigation: robust fallback)
- Memory usage could exceed estimates (mitigation: batch processing)
- Temporal indexing complexity (mitigation: simple sorted lists initially)