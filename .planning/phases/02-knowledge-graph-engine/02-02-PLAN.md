# Phase 2 Plan 2: Vector Embedding System

## Goal
Implement RotatE embeddings with temporal extensions for knowledge graph representation

## Context
- NetworkX temporal knowledge graph from Plan 02-01
- Need CPU-optimized embeddings (256 dimensions)
- Target: 100K-1M facts with efficient training

## Approach
Train RotatE base embeddings with HyTE temporal projections, store in Qdrant vector database

## Plan

### Task 1: RotatE Model Implementation
**Goal**: Build CPU-optimized RotatE embedding model

**Steps**:
1. Implement RotatE scoring function with complex-space rotations
2. Create entity and relation embedding layers (256 dimensions)
3. Add negative sampling strategy (4 negatives per positive)
4. Implement margin-based loss function
5. Add gradient clipping for training stability

**Verify**:
- Model forward pass completes for batch of 256 triples
- Complex rotation preserves norm (unit circle constraint)
- Loss decreases monotonically during initial training

### Task 2: Training Pipeline
**Goal**: Efficient training loop for CPU environments

**Steps**:
1. Create data loader for graph triples with batching
2. Implement training loop with Adam optimizer (lr=0.001)
3. Add exponential learning rate decay
4. Implement early stopping based on validation loss
5. Add checkpointing every 100 epochs

**Verify**:
- Training throughput > 10K triples/second on 8-core CPU
- 100K facts train in < 10 minutes for 500 epochs
- Checkpoints restore training state correctly

### Task 3: HyTE Temporal Extension
**Goal**: Add hyperplane-based temporal projections

**Steps**:
1. Create weekly temporal buckets (52 hyperplanes)
2. Implement orthogonal projection: e_τ = e - (w·e)w
3. Add temporal-aware scoring function
4. Integrate with base RotatE embeddings
5. Test projection preserves embedding quality

**Verify**:
- Projections maintain semantic similarity
- Temporal scoring differentiates time periods
- Memory overhead < 10MB for temporal parameters

### Task 4: Qdrant Vector Database Setup
**Goal**: Initialize and configure Qdrant for embedding storage

**Steps**:
1. Install and start Qdrant server (Docker or binary)
2. Create collections for entities and relations
3. Configure HNSW index with appropriate parameters
4. Set up payload schema for temporal metadata
5. Implement health check and connection pooling

**Verify**:
- Qdrant server responds on localhost:6333
- Collections created with 256-dim vectors
- Payload filtering works for temporal queries

### Task 5: Embedding Indexing Pipeline
**Goal**: Upload trained embeddings to Qdrant with metadata

**Steps**:
1. Extract embeddings from trained RotatE model
2. Apply HyTE projections for current time window
3. Prepare payload metadata (entity type, temporal bounds, confidence)
4. Batch upload vectors to Qdrant (1000 vectors per batch)
5. Create backup/restore mechanism

**Verify**:
- All entities and relations indexed successfully
- Search returns semantically similar entities
- Temporal filtering reduces search space correctly

### Task 6: Embedding Quality Validation
**Goal**: Verify embedding quality and performance

**Steps**:
1. Implement link prediction evaluation (MRR, Hits@K)
2. Test on held-out 10% of facts
3. Compare against TransE baseline
4. Measure inference latency for scoring
5. Visualize embedding clusters with t-SNE

**Verify**:
- MRR > 0.30 on test set (matches literature)
- RotatE outperforms TransE by >20%
- Inference < 1ms per triple
- Meaningful clusters in visualization

## Success Criteria
- [ ] RotatE training completes in < 10 minutes for 100K facts
- [ ] Embeddings achieve MRR > 0.30 on link prediction
- [ ] Qdrant indexes 100K vectors in < 30 seconds
- [ ] Temporal projections maintain quality (< 5% degradation)
- [ ] Total memory usage < 500MB including Qdrant

## Time Estimate
4-5 hours

## Dependencies
- NetworkX graph from Plan 02-01
- PyTorch (CPU version)
- Qdrant server and client
- NumPy for matrix operations

## Risk Factors
- Training instability with complex numbers (mitigation: gradient clipping)
- Qdrant setup complexity (mitigation: use Docker image)
- Embedding quality on sparse relations (mitigation: careful negative sampling)