---
phase: 07-bootstrap-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/bootstrap/__init__.py
  - src/bootstrap/orchestrator.py
  - src/bootstrap/stages.py
  - src/bootstrap/checkpoint.py
  - scripts/bootstrap.py
autonomous: true

must_haves:
  truths:
    - "Running scripts/bootstrap.py executes all five pipeline stages in order"
    - "Each stage wraps existing entry points without duplicating logic"
    - "Stage outputs are written to expected locations (data/gdelt/*, data/graphs/*, chroma_db/)"
  artifacts:
    - path: "src/bootstrap/__init__.py"
      provides: "Module exports"
      exports: ["StageOrchestrator", "CheckpointManager", "Stage"]
    - path: "src/bootstrap/orchestrator.py"
      provides: "Stage orchestration logic"
      contains: "class StageOrchestrator"
      min_lines: 60
    - path: "src/bootstrap/stages.py"
      provides: "Stage definitions wrapping existing entry points"
      contains: "class GDELTCollectStage"
      min_lines: 150
    - path: "src/bootstrap/checkpoint.py"
      provides: "Checkpoint state management"
      contains: "class CheckpointManager"
      min_lines: 50
    - path: "scripts/bootstrap.py"
      provides: "CLI entry point"
      contains: "def main"
      min_lines: 30
  key_links:
    - from: "src/bootstrap/stages.py"
      to: "src/training/data_collector.py"
      via: "import GDELTHistoricalCollector"
      pattern: "from.*data_collector.*import.*GDELTHistoricalCollector"
    - from: "src/bootstrap/stages.py"
      to: "src/training/data_processor.py"
      via: "import GDELTDataProcessor"
      pattern: "from.*data_processor.*import.*GDELTDataProcessor"
    - from: "src/bootstrap/stages.py"
      to: "src/knowledge_graph/graph_builder.py"
      via: "import TemporalKnowledgeGraph"
      pattern: "from.*graph_builder.*import.*TemporalKnowledgeGraph"
    - from: "src/bootstrap/stages.py"
      to: "src/forecasting/rag_pipeline.py"
      via: "import RAGPipeline"
      pattern: "from.*rag_pipeline.*import.*RAGPipeline"
---

<objective>
Create bootstrap orchestration module that chains existing pipeline components into a single-command system initialization.

Purpose: Enable zero-to-operational startup via `uv run python scripts/bootstrap.py` that executes GDELT collection, data processing, graph construction, graph persistence, and RAG indexing.

Output: `src/bootstrap/` module with orchestrator, stages, checkpoint manager, and CLI entry point.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-bootstrap-pipeline/07-RESEARCH.md

# Existing entry points to wrap
@src/training/data_collector.py
@src/training/data_processor.py
@src/knowledge_graph/graph_builder.py
@src/knowledge_graph/persistence.py
@src/forecasting/rag_pipeline.py
@src/database/storage.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create bootstrap module with checkpoint manager and orchestrator</name>
  <files>
    src/bootstrap/__init__.py
    src/bootstrap/checkpoint.py
    src/bootstrap/orchestrator.py
  </files>
  <action>
Create `src/bootstrap/` directory and implement core infrastructure:

**src/bootstrap/checkpoint.py:**
- `StageStatus` enum: PENDING, RUNNING, COMPLETED, FAILED
- `StageState` dataclass: name, status, started_at, completed_at, output_path, error
- `BootstrapState` dataclass: stages dict, last_updated timestamp
- `CheckpointManager` class:
  - `__init__(self, state_file: Path)` - default: `data/bootstrap_state.json`
  - `load() -> BootstrapState` - returns empty state if file missing
  - `save(state: BootstrapState)` - atomic write using tempfile + os.replace()
  - `mark_running(stage_name)`, `mark_completed(stage_name, output_path)`, `mark_failed(stage_name, error)`
  - `get_status(stage_name) -> StageStatus`

**src/bootstrap/orchestrator.py:**
- `Stage` Protocol: name property, run() method, validate_output() method, get_output_path() method
- `StageOrchestrator` class:
  - `__init__(self, checkpoint_manager: CheckpointManager, reporter: ProgressReporter)`
  - `register_stage(stage: Stage)` - adds to ordered stage list
  - `run_all()` - executes stages in order, respects checkpoint state
  - `should_skip(stage) -> bool` - dual check: checkpoint COMPLETED AND output exists
  - `execute_stage(stage)` - marks running, calls run(), validates, marks completed/failed

**src/bootstrap/__init__.py:**
- Export: StageOrchestrator, CheckpointManager, Stage, StageStatus, BootstrapState

Use Python 3.11+ features: dataclasses, Enum, Protocol, type hints. Use `from __future__ import annotations` for forward references.

Critical: Atomic state file write pattern from research - write to temp file, then `os.replace()` to prevent corruption on interrupt.
  </action>
  <verify>
```bash
cd /home/kondraki/personal/geopol
python -c "from src.bootstrap import StageOrchestrator, CheckpointManager, Stage, StageStatus, BootstrapState; print('Imports OK')"
```
  </verify>
  <done>
Bootstrap module imports successfully with all core classes available.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement stage definitions wrapping existing entry points</name>
  <files>
    src/bootstrap/stages.py
  </files>
  <action>
Create stage implementations that wrap existing pipeline components. Each stage implements the Stage protocol.

**Critical gap to bridge:** Graph builder reads SQLite but processor outputs Parquet. Implement Parquet-to-SQLite loading in ProcessEventsStage output handling.

**GDELTCollectStage:**
- Wraps `GDELTHistoricalCollector.collect_last_n_days(n_days=30, quad_classes=[1,2,3,4])`
- Output path: `data/gdelt/raw/` directory
- validate_output(): Check at least one `gdelt_*.csv` file exists and is non-empty

**ProcessEventsStage:**
- Wraps `GDELTDataProcessor.process_all()`
- Output path: `data/gdelt/processed/events.parquet`
- validate_output(): Check parquet exists and has rows (use pandas)
- **Post-processing step:** Load parquet into EventStorage (SQLite) for graph builder:
  ```python
  # After processor.process_all()
  df = pd.read_parquet(output_path)
  storage = EventStorage(db_path="data/events.db")
  events = [Event.from_tkg_row(row) for _, row in df.iterrows()]
  storage.insert_events(events)
  ```
  This bridges the Parquet->SQLite gap identified in research.

**BuildGraphStage:**
- Uses `TemporalKnowledgeGraph()` and `graph.add_events_batch(db_path="data/events.db")`
- Returns in-memory graph object (stored in orchestrator context for next stage)
- validate_output(): Graph has > 0 nodes and > 0 edges

**PersistGraphStage:**
- Uses `GraphPersistence(graph.graph).save(output_path, format='graphml')`
- Output path: `data/graphs/knowledge_graph.graphml`
- validate_output(): File exists, non-zero size, node count > 0

**IndexRAGStage:**
- Uses `RAGPipeline(persist_dir="./chroma_db").index_graph_patterns(graph, rebuild=True)`
- Output path: `chroma_db/` directory
- validate_output(): ChromaDB collection exists with document count > 0

Each stage:
- Constructor takes optional config overrides
- `run()` returns dict with stats (events_count, duration_seconds, etc.)
- `validate_output()` returns (bool, str) tuple - (is_valid, reason)
- Logs progress via standard logging module

Handle the stage dependency for graph object passing: BuildGraphStage stores graph in a context dict, PersistGraphStage and IndexRAGStage read from context.
  </action>
  <verify>
```bash
cd /home/kondraki/personal/geopol
python -c "
from src.bootstrap.stages import (
    GDELTCollectStage, ProcessEventsStage, BuildGraphStage,
    PersistGraphStage, IndexRAGStage
)
print('GDELTCollectStage:', GDELTCollectStage().name)
print('ProcessEventsStage:', ProcessEventsStage().name)
print('BuildGraphStage:', BuildGraphStage().name)
print('PersistGraphStage:', PersistGraphStage().name)
print('IndexRAGStage:', IndexRAGStage().name)
print('All stages import OK')
"
```
  </verify>
  <done>
All five stage classes import and instantiate successfully, each reporting its name property.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create CLI entry point script</name>
  <files>
    scripts/bootstrap.py
  </files>
  <action>
Create `scripts/bootstrap.py` as the single-command entry point.

**Script structure:**
```python
#!/usr/bin/env python
"""
Bootstrap the geopolitical forecasting system from zero data to operational.

Usage:
    uv run python scripts/bootstrap.py [--force-stage STAGE] [--dry-run]

Stages:
    1. collect   - Fetch GDELT events (30 days)
    2. process   - Transform to TKG format + load into SQLite
    3. graph     - Build temporal knowledge graph
    4. persist   - Save graph to GraphML
    5. index     - Index patterns in RAG store
"""
```

**Implementation:**
- Parse args: `--force-stage NAME` to re-run specific stage, `--dry-run` to show what would run
- Set up logging to stdout with `[STAGE] name: message` format (ConsoleReporter pattern from research)
- Create CheckpointManager with default state file
- Create StageOrchestrator with checkpoint manager
- Register all 5 stages in order
- Call `orchestrator.run_all()`
- Print final summary: stages completed, total duration, paths to outputs

**Console output format (matching research pattern):**
```
[STAGE] collect: Starting...
[STAGE] collect: Fetching 30 days of GDELT events
[STAGE] collect: Completed in 45.2s (15,234 events)
[STAGE] process: Starting...
...
[BOOTSTRAP] Complete: 5/5 stages in 312.4s
[BOOTSTRAP] Outputs:
  - Events: data/gdelt/processed/events.parquet (15,234 rows)
  - Graph: data/graphs/knowledge_graph.graphml (1,234 nodes, 5,678 edges)
  - RAG Index: chroma_db/ (892 patterns)
```

Add `if __name__ == "__main__": main()` guard.
  </action>
  <verify>
```bash
cd /home/kondraki/personal/geopol
python scripts/bootstrap.py --dry-run
```
Expected: Shows stages that would run without executing them.
  </verify>
  <done>
`scripts/bootstrap.py --dry-run` executes without errors and displays the 5 stages that would be run.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Module structure exists:
```bash
ls -la src/bootstrap/
# Should show: __init__.py, checkpoint.py, orchestrator.py, stages.py
```

2. All imports work:
```bash
python -c "from src.bootstrap import StageOrchestrator, CheckpointManager; from src.bootstrap.stages import GDELTCollectStage"
```

3. Dry run works:
```bash
python scripts/bootstrap.py --dry-run
```

4. Type checking passes (if mypy available):
```bash
mypy src/bootstrap/ --ignore-missing-imports || echo "mypy not configured"
```
</verification>

<success_criteria>
- `src/bootstrap/` module exists with 4 Python files
- All stage classes wrap existing entry points (no logic duplication)
- ProcessEventsStage includes Parquet->SQLite bridge
- `scripts/bootstrap.py --dry-run` shows all 5 stages
- No new external dependencies added (all stdlib + existing packages)
</success_criteria>

<output>
After completion, create `.planning/phases/07-bootstrap-pipeline/07-01-SUMMARY.md`
</output>
