---
phase: 07-bootstrap-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/bootstrap/checkpoint.py
  - src/bootstrap/orchestrator.py
  - src/bootstrap/validation.py
  - tests/test_bootstrap.py
autonomous: true

must_haves:
  truths:
    - "Re-running bootstrap skips stages marked complete with valid outputs"
    - "Interrupted bootstrap resumes from last successful checkpoint on re-run"
    - "Each stage reports progress to stdout during execution"
    - "Invalid or missing outputs cause stage to re-run despite checkpoint status"
  artifacts:
    - path: "src/bootstrap/validation.py"
      provides: "Output validation functions for each stage"
      contains: "def validate_gdelt_output"
      min_lines: 60
    - path: "src/bootstrap/checkpoint.py"
      provides: "Enhanced checkpoint with dual idempotency"
      contains: "def should_skip_stage"
      min_lines: 80
    - path: "tests/test_bootstrap.py"
      provides: "Tests for checkpoint/resume behavior"
      contains: "def test_resume_from_checkpoint"
      min_lines: 80
  key_links:
    - from: "src/bootstrap/orchestrator.py"
      to: "src/bootstrap/validation.py"
      via: "import validation functions"
      pattern: "from.*validation.*import"
    - from: "src/bootstrap/orchestrator.py"
      to: "src/bootstrap/checkpoint.py"
      via: "dual idempotency check"
      pattern: "should_skip_stage"
---

<objective>
Implement checkpoint/resume logic with dual idempotency checks (state file + output validation) ensuring bootstrap is fully resumable and skips completed work.

Purpose: Satisfy success criteria 2-4 from ROADMAP: skip completed stages on re-run, resume from checkpoint after interrupt, report progress for each stage.

Output: Enhanced checkpoint manager with idempotency, validation module, progress reporting, and tests proving resume behavior.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-bootstrap-pipeline/07-RESEARCH.md
@.planning/phases/07-bootstrap-pipeline/07-01-SUMMARY.md

# Files from Plan 01 to enhance
@src/bootstrap/checkpoint.py
@src/bootstrap/orchestrator.py
@src/bootstrap/stages.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement output validation module</name>
  <files>
    src/bootstrap/validation.py
  </files>
  <action>
Create `src/bootstrap/validation.py` with stage-specific output validators.

Each validator implements dual idempotency: check output exists AND is valid (not empty/corrupted).

**Functions:**

```python
def validate_gdelt_output(output_dir: Path) -> tuple[bool, str]:
    """
    Validate GDELT collection output.

    Checks:
    1. Directory exists
    2. At least one gdelt_*.csv file exists
    3. At least one CSV has > 0 bytes

    Returns (is_valid, reason)
    """

def validate_processed_output(parquet_path: Path, sqlite_path: Path) -> tuple[bool, str]:
    """
    Validate processed events output.

    Checks:
    1. Parquet file exists and is readable
    2. Parquet has > 0 rows
    3. SQLite database exists
    4. SQLite events table has > 0 rows

    Returns (is_valid, reason)
    """

def validate_graph_output(graphml_path: Path) -> tuple[bool, str]:
    """
    Validate persisted graph output.

    Checks:
    1. GraphML file exists
    2. File size > 0
    3. Can be loaded by networkx (basic parse test)
    4. Loaded graph has > 0 nodes

    Returns (is_valid, reason)
    """

def validate_rag_output(chroma_dir: Path, collection_name: str = "graph_patterns") -> tuple[bool, str]:
    """
    Validate RAG index output.

    Checks:
    1. ChromaDB directory exists
    2. Collection exists
    3. Collection has > 0 documents

    Returns (is_valid, reason)
    """
```

**Implementation notes:**
- Use lazy imports (pandas, networkx, chromadb) inside functions to avoid import-time failures
- Catch exceptions and return (False, error_message) rather than raising
- Log validation results at DEBUG level
- Be defensive: file might be locked, partially written, or corrupted

**Pitfall handling (from research):**
- Stale empty graph file: Check node count, not just file existence
- Partial RAG index: Check document count in collection
- Database locks: Use short-lived connections with timeout
  </action>
  <verify>
```bash
cd /home/kondraki/personal/geopol
python -c "
from src.bootstrap.validation import (
    validate_gdelt_output, validate_processed_output,
    validate_graph_output, validate_rag_output
)
from pathlib import Path

# Test with non-existent paths - should return False
valid, reason = validate_gdelt_output(Path('/nonexistent'))
assert not valid, 'Should fail for missing dir'
print(f'validate_gdelt_output: {valid}, {reason}')

valid, reason = validate_graph_output(Path('/nonexistent.graphml'))
assert not valid, 'Should fail for missing file'
print(f'validate_graph_output: {valid}, {reason}')

print('Validation functions work correctly')
"
```
  </verify>
  <done>
Validation functions return (False, reason) for non-existent/invalid outputs without raising exceptions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance checkpoint manager with dual idempotency and progress reporting</name>
  <files>
    src/bootstrap/checkpoint.py
    src/bootstrap/orchestrator.py
  </files>
  <action>
**Enhance src/bootstrap/checkpoint.py:**

Add `should_skip_stage` function implementing dual idempotency:

```python
def should_skip_stage(
    stage_name: str,
    state: BootstrapState,
    validator: Callable[[],  tuple[bool, str]]
) -> tuple[bool, str]:
    """
    Dual idempotency check: checkpoint status AND output validation.

    Returns (should_skip, reason) where reason explains decision.

    Logic:
    1. If checkpoint says PENDING/RUNNING/FAILED -> don't skip
    2. If checkpoint says COMPLETED but output invalid -> don't skip (log warning)
    3. If checkpoint says COMPLETED and output valid -> skip
    """
```

Add `ProgressReporter` protocol and `ConsoleReporter` implementation:

```python
class ProgressReporter(Protocol):
    def stage_start(self, name: str) -> None: ...
    def stage_progress(self, name: str, message: str) -> None: ...
    def stage_complete(self, name: str, duration_sec: float, stats: dict) -> None: ...
    def stage_error(self, name: str, error: str) -> None: ...
    def stage_skipped(self, name: str, reason: str) -> None: ...

class ConsoleReporter:
    """Reports progress to stdout with [STAGE] prefix format.

    IMPORTANT: ALL output (including errors) goes to stdout per success criterion #4.
    """

    def stage_start(self, name: str) -> None:
        print(f"[STAGE] {name}: Starting...", flush=True)

    def stage_progress(self, name: str, message: str) -> None:
        print(f"[STAGE] {name}: {message}", flush=True)

    def stage_complete(self, name: str, duration_sec: float, stats: dict) -> None:
        summary = ", ".join(f"{k}={v}" for k, v in stats.items() if k != "duration_seconds")
        print(f"[STAGE] {name}: Completed in {duration_sec:.1f}s ({summary})", flush=True)

    def stage_error(self, name: str, error: str) -> None:
        # Write to STDOUT (not stderr) per success criterion:
        # "The bootstrap script reports progress for each stage (stage name, status, errors if any) to stdout"
        print(f"[STAGE] {name}: FAILED - {error}", flush=True)

    def stage_skipped(self, name: str, reason: str) -> None:
        print(f"[STAGE] {name}: Skipped ({reason})", flush=True)
```

**Enhance src/bootstrap/orchestrator.py:**

Update `StageOrchestrator.run_all()` to:
1. For each stage, call `should_skip_stage()` with appropriate validator
2. If skip, call `reporter.stage_skipped()` and continue
3. If not skip, mark RUNNING, execute, validate, mark COMPLETED/FAILED
4. On any exception, mark FAILED with error message

Add stage-to-validator mapping using validation module functions.

Ensure state is saved after EACH stage completes (not just at end) - critical for interrupt recovery.

**Resume behavior:**
- If stage marked RUNNING (interrupted mid-execution): re-run it
- If stage marked FAILED: re-run it
- If stage marked COMPLETED but output invalid: re-run it (log warning about stale checkpoint)
- If stage marked COMPLETED and output valid: skip it
  </action>
  <verify>
```bash
cd /home/kondraki/personal/geopol
python -c "
from src.bootstrap.checkpoint import (
    CheckpointManager, BootstrapState, StageStatus,
    should_skip_stage, ConsoleReporter, ProgressReporter
)

# Test ConsoleReporter
reporter = ConsoleReporter()
reporter.stage_start('test')
reporter.stage_progress('test', 'doing work')
reporter.stage_complete('test', 1.5, {'events': 100})
reporter.stage_skipped('test2', 'already complete')

# Test should_skip_stage with completed stage but missing output
state = BootstrapState()
state.stages['test'] = type('Stage', (), {'status': StageStatus.COMPLETED})()
skip, reason = should_skip_stage('test', state, lambda: (False, 'file missing'))
assert not skip, 'Should not skip when output invalid'
print(f'should_skip_stage (invalid output): skip={skip}, reason={reason}')

print('Enhanced checkpoint functions work correctly')
"
```
  </verify>
  <done>
ConsoleReporter outputs [STAGE] formatted messages to stdout (including errors), and `should_skip_stage` correctly refuses to skip when output validation fails.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add tests for checkpoint/resume behavior</name>
  <files>
    tests/test_bootstrap.py
  </files>
  <action>
Create `tests/test_bootstrap.py` with tests proving resume and idempotency behavior.

**Test cases:**

```python
import pytest
from pathlib import Path
import tempfile
import json

from src.bootstrap.checkpoint import (
    CheckpointManager, BootstrapState, StageStatus, StageState,
    should_skip_stage, ConsoleReporter
)
from src.bootstrap.validation import validate_gdelt_output


class TestCheckpointManager:
    def test_load_missing_file_returns_empty_state(self, tmp_path):
        """Loading from non-existent file returns empty state."""
        mgr = CheckpointManager(tmp_path / "state.json")
        state = mgr.load()
        assert len(state.stages) == 0

    def test_save_and_load_roundtrip(self, tmp_path):
        """State survives save/load cycle."""
        mgr = CheckpointManager(tmp_path / "state.json")
        state = BootstrapState()
        state.stages["test"] = StageState(
            name="test",
            status=StageStatus.COMPLETED,
            output_path="/some/path"
        )
        mgr.save(state)

        loaded = mgr.load()
        assert "test" in loaded.stages
        assert loaded.stages["test"].status == StageStatus.COMPLETED

    def test_atomic_write_creates_file(self, tmp_path):
        """Atomic write creates state file."""
        state_file = tmp_path / "state.json"
        mgr = CheckpointManager(state_file)
        mgr.save(BootstrapState())
        assert state_file.exists()


class TestDualIdempotency:
    def test_skip_when_complete_and_valid(self, tmp_path):
        """Skip stage when checkpoint=COMPLETED and output valid."""
        state = BootstrapState()
        state.stages["test"] = StageState(name="test", status=StageStatus.COMPLETED)

        skip, reason = should_skip_stage("test", state, lambda: (True, "ok"))
        assert skip is True
        assert "already complete" in reason.lower()

    def test_no_skip_when_complete_but_invalid(self, tmp_path):
        """Don't skip when checkpoint=COMPLETED but output missing/invalid."""
        state = BootstrapState()
        state.stages["test"] = StageState(name="test", status=StageStatus.COMPLETED)

        skip, reason = should_skip_stage("test", state, lambda: (False, "file missing"))
        assert skip is False
        assert "invalid" in reason.lower() or "missing" in reason.lower()

    def test_no_skip_when_pending(self):
        """Don't skip when checkpoint=PENDING."""
        state = BootstrapState()
        state.stages["test"] = StageState(name="test", status=StageStatus.PENDING)

        skip, reason = should_skip_stage("test", state, lambda: (True, "ok"))
        assert skip is False

    def test_no_skip_when_running(self):
        """Don't skip when checkpoint=RUNNING (interrupted)."""
        state = BootstrapState()
        state.stages["test"] = StageState(name="test", status=StageStatus.RUNNING)

        skip, reason = should_skip_stage("test", state, lambda: (True, "ok"))
        assert skip is False
        assert "interrupt" in reason.lower() or "running" in reason.lower()

    def test_no_skip_when_failed(self):
        """Don't skip when checkpoint=FAILED."""
        state = BootstrapState()
        state.stages["test"] = StageState(name="test", status=StageStatus.FAILED)

        skip, reason = should_skip_stage("test", state, lambda: (True, "ok"))
        assert skip is False


class TestValidation:
    def test_gdelt_validation_fails_for_empty_dir(self, tmp_path):
        """GDELT validation fails for directory with no CSVs."""
        valid, reason = validate_gdelt_output(tmp_path)
        assert valid is False
        assert "no csv" in reason.lower() or "empty" in reason.lower()

    def test_gdelt_validation_passes_with_csv(self, tmp_path):
        """GDELT validation passes when CSV exists with content."""
        csv_file = tmp_path / "gdelt_2025-01-01.csv"
        csv_file.write_text("col1,col2\nval1,val2\n")

        valid, reason = validate_gdelt_output(tmp_path)
        assert valid is True


class TestResumeScenario:
    def test_resume_from_checkpoint(self, tmp_path):
        """Simulate interrupt and resume scenario."""
        state_file = tmp_path / "state.json"
        mgr = CheckpointManager(state_file)

        # Simulate: stages 1-2 completed, stage 3 was running (interrupted)
        state = BootstrapState()
        state.stages["collect"] = StageState(name="collect", status=StageStatus.COMPLETED)
        state.stages["process"] = StageState(name="process", status=StageStatus.COMPLETED)
        state.stages["graph"] = StageState(name="graph", status=StageStatus.RUNNING)
        mgr.save(state)

        # Resume: load state, check which stages to run
        loaded = mgr.load()

        # collect and process should skip (if outputs valid)
        skip_collect, _ = should_skip_stage("collect", loaded, lambda: (True, "ok"))
        skip_process, _ = should_skip_stage("process", loaded, lambda: (True, "ok"))
        skip_graph, _ = should_skip_stage("graph", loaded, lambda: (True, "ok"))

        assert skip_collect is True, "Should skip completed stage"
        assert skip_process is True, "Should skip completed stage"
        assert skip_graph is False, "Should NOT skip interrupted stage"
```

Run tests with: `pytest tests/test_bootstrap.py -v`
  </action>
  <verify>
```bash
cd /home/kondraki/personal/geopol
pytest tests/test_bootstrap.py -v --tb=short
```
Expected: All tests pass.
  </verify>
  <done>
All bootstrap tests pass, proving checkpoint/resume and dual idempotency behavior works correctly.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Validation module works:
```bash
python -c "from src.bootstrap.validation import validate_gdelt_output, validate_graph_output"
```

2. Enhanced checkpoint works:
```bash
python -c "from src.bootstrap.checkpoint import should_skip_stage, ConsoleReporter"
```

3. All tests pass:
```bash
pytest tests/test_bootstrap.py -v
```

4. Full bootstrap dry-run still works:
```bash
python scripts/bootstrap.py --dry-run
```

5. Manual resume test (if data exists):
```bash
# Run bootstrap, interrupt with Ctrl+C, re-run - should skip completed stages
```
</verification>

<success_criteria>
- `should_skip_stage()` implements dual idempotency (checkpoint + output validation)
- Validation functions check actual output validity, not just existence
- ConsoleReporter outputs ALL progress (including errors) to stdout
- All tests in test_bootstrap.py pass
- Re-running bootstrap skips stages with valid outputs
- Interrupted bootstrap (RUNNING state) re-runs that stage
</success_criteria>

<output>
After completion, create `.planning/phases/07-bootstrap-pipeline/07-02-SUMMARY.md`
</output>
