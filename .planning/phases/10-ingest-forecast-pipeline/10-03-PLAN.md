---
phase: 10-ingest-forecast-pipeline
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ingest/feed_config.py
  - src/ingest/article_processor.py
  - src/ingest/rss_daemon.py
  - scripts/rss_daemon.py
  - deploy/systemd/geopol-rss-daemon.service
  - tests/test_rss_daemon.py
autonomous: true

must_haves:
  truths:
    - "RSS daemon polls tier-1 feeds every 15 minutes and tier-2 feeds every hour"
    - "Articles are extracted via trafilatura and chunked on paragraph boundaries"
    - "Article chunks are indexed into a separate ChromaDB collection using the same embedding model (all-mpnet-base-v2) as the existing graph_patterns collection"
    - "Duplicate articles (same URL) are skipped via ChromaDB metadata check"
    - "Articles older than 90 days are pruned from the RSS collection"
    - "IngestRun rows with daemon_type='rss' track per-cycle metrics"
  artifacts:
    - path: "src/ingest/feed_config.py"
      provides: "Tiered feed list extracted from WM's feeds.ts, filtered for geopolitical relevance"
      min_lines: 100
    - path: "src/ingest/article_processor.py"
      provides: "trafilatura extraction + semantic paragraph chunking + ChromaDB indexing"
      min_lines: 80
    - path: "src/ingest/rss_daemon.py"
      provides: "Async RSS polling daemon with tiered scheduling"
      min_lines: 150
    - path: "tests/test_rss_daemon.py"
      provides: "Unit tests for RSS processing pipeline"
      min_lines: 80
  key_links:
    - from: "src/ingest/rss_daemon.py"
      to: "src/ingest/article_processor.py"
      via: "process_article() called per fetched article"
      pattern: "process_article|extract_and_chunk"
    - from: "src/ingest/article_processor.py"
      to: "chromadb"
      via: "ChromaDB collection.add() for article chunks"
      pattern: "collection\\.add|chroma"
    - from: "src/ingest/rss_daemon.py"
      to: "src/db/models.py"
      via: "IngestRun with daemon_type='rss'"
      pattern: "daemon_type.*rss"
---

<objective>
Build the RSS feed ingestion daemon that polls geopolitical news sources at tiered intervals, extracts article text via trafilatura, chunks on paragraph boundaries, and indexes into a separate ChromaDB collection for RAG enrichment. This gives the LLM full narrative context beyond GDELT's terse event codes.

Purpose: GDELT provides structured event triples (actor, verb, actor) but lacks narrative context. RSS articles provide the "why" behind events -- crucial for Gemini's reasoning quality when generating forecasts.

Output: RSS daemon (`scripts/rss_daemon.py`), feed configuration, article processor, ChromaDB indexing, systemd unit file, unit tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-ingest-forecast-pipeline/10-CONTEXT.md
@.planning/phases/10-ingest-forecast-pipeline/10-RESEARCH.md
@src/forecasting/rag_pipeline.py
@src/settings.py
@src/db/models.py
@/home/kondraki/personal/worldmonitor/src/config/feeds.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Feed configuration and article processor</name>
  <files>
    src/ingest/feed_config.py
    src/ingest/article_processor.py
  </files>
  <action>
  **src/ingest/feed_config.py**:

  1. Extract geopolitical RSS feed URLs from WM's `/home/kondraki/personal/worldmonitor/src/config/feeds.ts`. Parse the TypeScript file to get raw RSS URLs (strip any WM proxy prefix like `/api/rss-proxy?url=`).

  2. Filter to geopolitical feeds only. Remove: tech news (TechCrunch, Hacker News, etc.), startup/VC feeds, gaming feeds, entertainment. Keep: wire services, government, intl orgs, defense, geopolitics, regional conflict analysis.

  3. Define two tiers:
     ```python
     @dataclass
     class FeedSource:
         name: str
         url: str
         tier: int  # 1 = 15-min, 2 = hourly
         category: str  # "wire", "government", "intl_org", "defense", "analysis", "regional"

     TIER_1_FEEDS: list[FeedSource]  # ~50 major outlets
     TIER_2_FEEDS: list[FeedSource]  # remaining geopolitical feeds
     ALL_FEEDS: list[FeedSource]     # combined
     ```

  4. Tier 1 should include (from RESEARCH.md): Reuters, AP, AFP, Bloomberg, White House, State Dept, Pentagon, UN News, BBC World, BBC Middle East, Guardian World, Al Jazeera, Financial Times, CNN World, France 24, DW News, Military Times, USNI News, Tagesschau, ANSA, NOS Nieuws, SVT Nyheter, UK MOD, IAEA, WHO, UNHCR, and other top-tier geopolitical sources.

  5. If the WM feeds.ts doesn't contain enough geopolitical feeds, supplement with well-known RSS feeds for the missing outlets listed above. Each entry gets a `name`, `url`, `tier`, `category`.

  **src/ingest/article_processor.py**:

  1. `extract_article_text(html: str, url: str) -> str | None`:
     - Use trafilatura with the configuration from RESEARCH.md: `output_format="txt"`, `include_comments=False`, `include_tables=False`, `favor_precision=True`, `deduplicate=True`.
     - Return None if extraction fails or text < 200 chars.

  2. `chunk_article(text: str, max_tokens: int = 200) -> list[str]`:
     - Semantic chunking on paragraph boundaries (double newline split).
     - Per RESEARCH.md: if paragraph exceeds max_tokens, fall back to sentence splitting.
     - Rough token estimate: `len(words) * 1.3`.

  3. `ArticleIndexer` class:
     - Constructor takes `persist_dir: str = "./chroma_db"`, `collection_name: str = "rss_articles"`.
     - Uses `chromadb.PersistentClient` (same persist_dir as existing RAG pipeline).
     - Creates/gets collection `rss_articles` (separate from existing `graph_patterns` per CONTEXT.md).
     - **CRITICAL**: Use `sentence-transformers/all-mpnet-base-v2` for embeddings -- must match existing collection. Use `chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-mpnet-base-v2")` when creating the collection.

  4. `async index_article(url: str, title: str, text: str, published: str | None, source_name: str) -> int`:
     - Check deduplication: `collection.get(where={"url": url})`. If exists, return 0.
     - Chunk the text.
     - Add chunks to ChromaDB with metadata: `{"url": url, "title": title, "source": source_name, "published": published, "chunk_index": i}`.
     - IDs: `f"{hash(url)[:12]}_{i}"` for each chunk.
     - Return number of chunks indexed.

  5. `async prune_old_articles(retention_days: int = 90) -> int`:
     - Query ChromaDB for articles with `published` older than retention_days.
     - Delete matching IDs.
     - Return count deleted.
     - Handle missing/malformed published dates gracefully (skip, don't delete).
  </action>
  <verify>
  `python -c "from src.ingest.feed_config import TIER_1_FEEDS, TIER_2_FEEDS; print(f'T1: {len(TIER_1_FEEDS)}, T2: {len(TIER_2_FEEDS)}')"` prints counts (T1 ~50, T2 varies).
  `python -c "from src.ingest.article_processor import chunk_article; chunks = chunk_article('Para 1.\\n\\nPara 2.\\n\\nPara 3.'); print(len(chunks))"` prints chunk count.
  `python -c "from src.ingest.article_processor import ArticleIndexer; print('OK')"` succeeds.
  </verify>
  <done>
  Feed config has tiered geopolitical sources extracted from WM's feeds.ts. Article processor extracts text via trafilatura, chunks on paragraph boundaries, and indexes into ChromaDB with deduplication and metadata. Embedding model matches existing collection.
  </done>
</task>

<task type="auto">
  <name>Task 2: RSS polling daemon with tiered scheduling and systemd unit</name>
  <files>
    src/ingest/rss_daemon.py
    scripts/rss_daemon.py
    deploy/systemd/geopol-rss-daemon.service
  </files>
  <action>
  **src/ingest/rss_daemon.py**:

  Build the RSS polling daemon as an independent process (per CONTEXT.md decision: two separate OS processes for fault isolation).

  1. `RSSDaemon` class:
     - Constructor takes tier1_interval (default 900s), tier2_interval (default 3600s), article_indexer (ArticleIndexer).
     - `_shutdown: asyncio.Event` for graceful SIGTERM (same pattern as GDELT poller).
     - `_last_poll_tier1: datetime | None` and `_last_poll_tier2: datetime | None` for scheduling.

  2. `async run()` method -- main event loop:
     - Register SIGTERM/SIGINT handlers.
     - Loop while not shutdown:
       a. Check if tier 1 is due (elapsed >= tier1_interval or first run). If so, poll all TIER_1_FEEDS.
       b. Check if tier 2 is due (elapsed >= tier2_interval or first run). If so, poll all TIER_2_FEEDS.
       c. Wait 60 seconds (poll check interval) or until shutdown.
     - On exit: log "RSS daemon shut down cleanly".

  3. `async _poll_feeds(feeds: list[FeedSource]) -> tuple[int, int, int]`:
     - For each feed, fetch RSS content via `aiohttp`.
     - Parse with `feedparser`.
     - For each entry in the feed:
       a. Extract URL from entry.link.
       b. Fetch the article HTML via `aiohttp` (separate from the RSS fetch).
       c. Extract text via `extract_article_text()`.
       d. Index via `article_indexer.index_article()`.
     - Return (articles_fetched, articles_indexed, articles_skipped_duplicate).
     - Use `asyncio.gather` with `return_exceptions=True` to fetch multiple feeds concurrently (bounded by semaphore, max 10 concurrent fetches to avoid overwhelming sources).
     - Handle individual feed failures gracefully -- log warning and continue to next feed.

  4. `async _record_run(...)`: create IngestRun with `daemon_type="rss"`, recording articles_fetched, articles_indexed (as events_new), articles_skipped (as events_duplicate).

  5. Periodic pruning: every 24 hours, call `article_indexer.prune_old_articles()` to remove articles older than `Settings.rss_article_retention_days`.

  **scripts/rss_daemon.py**:
  - Entry point: parse CLI args (--tier1-interval, --tier2-interval).
  - Initialize logging, PostgreSQL, create ArticleIndexer, create RSSDaemon, run.

  **deploy/systemd/geopol-rss-daemon.service**:
  - Similar to GDELT poller service but for RSS daemon.
  - Separate unit for fault isolation.
  - MemoryMax=1G (article processing is more memory-intensive than GDELT CSV parsing).
  </action>
  <verify>
  `python -c "from src.ingest.rss_daemon import RSSDaemon; print('OK')"` succeeds.
  `python scripts/rss_daemon.py --help` shows usage.
  `ls deploy/systemd/geopol-rss-daemon.service` exists.
  </verify>
  <done>
  RSS daemon polls tier-1 feeds every 15 min and tier-2 every hour. Articles are fetched, extracted, chunked, and indexed into ChromaDB. Duplicate articles skipped. Per-run metrics recorded as IngestRun with daemon_type="rss". Periodic 90-day pruning. systemd unit file for independent process management.
  </done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for RSS processing pipeline</name>
  <files>
    tests/test_rss_daemon.py
  </files>
  <action>
  Write unit tests mocking external I/O (aiohttp, feedparser, ChromaDB, trafilatura).

  Tests to include:

  1. **test_chunk_article_paragraph_boundaries**: Verify text with 3 paragraphs produces 3 chunks (under max_tokens).
  2. **test_chunk_article_large_paragraph_splits**: Verify a single 500-word paragraph gets split by sentences.
  3. **test_extract_article_text_success**: Mock trafilatura.extract returning valid text, verify return value.
  4. **test_extract_article_text_too_short**: Mock trafilatura returning < 200 chars, verify None returned.
  5. **test_article_dedup_by_url**: Mock ChromaDB.get returning existing URL, verify index_article returns 0 (skipped).
  6. **test_article_indexing_new**: Mock ChromaDB.get returning empty, verify collection.add called with correct metadata.
  7. **test_feed_config_tiers**: Verify TIER_1_FEEDS has ~50 entries, all have tier=1. TIER_2_FEEDS has tier=2.
  8. **test_feed_config_no_non_geopolitical**: Verify no feed in ALL_FEEDS has category in ["tech", "startup", "gaming", "entertainment"].
  9. **test_rss_daemon_shutdown**: Create daemon, set shutdown event, verify run() exits cleanly.
  10. **test_poll_feeds_concurrent_limit**: Verify semaphore limits concurrent fetches to 10.

  Use `pytest`, mock aiohttp responses, mock ChromaDB client.
  </action>
  <verify>
  `uv run pytest tests/test_rss_daemon.py -v` -- all 10 tests pass, no skips.
  </verify>
  <done>
  10 tests covering chunking, extraction, deduplication, indexing, feed config validation, daemon lifecycle, and concurrency limits. All pass without external dependencies.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.ingest.feed_config import ALL_FEEDS; print(f'{len(ALL_FEEDS)} feeds configured')"` shows feed count.
2. `python -c "from src.ingest.article_processor import ArticleIndexer, chunk_article; print('OK')"` succeeds.
3. `python -c "from src.ingest.rss_daemon import RSSDaemon; print('OK')"` succeeds.
4. `uv run pytest tests/test_rss_daemon.py -v` -- all tests pass.
5. RSS ChromaDB collection uses `all-mpnet-base-v2` embeddings (matching existing graph_patterns collection).
</verification>

<success_criteria>
- RSS daemon polls tiered feeds with independent scheduling (15min T1, 1h T2).
- Articles extracted via trafilatura, chunked on paragraph boundaries, indexed in separate ChromaDB collection.
- Deduplication by URL prevents redundant indexing across polling cycles.
- 90-day rolling window prunes old articles.
- Per-run metrics tracked in IngestRun with daemon_type="rss".
- Requirement INGEST-06 is fully covered.
</success_criteria>

<output>
After completion, create `.planning/phases/10-ingest-forecast-pipeline/10-03-SUMMARY.md`
</output>
