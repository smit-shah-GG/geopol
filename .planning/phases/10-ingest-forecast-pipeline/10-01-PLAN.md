---
phase: 10-ingest-forecast-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/settings.py
  - src/db/models.py
  - alembic/versions/20260301_002_pending_questions_and_daemon_type.py
  - src/ingest/__init__.py
  - src/ingest/gdelt_poller.py
  - src/ingest/backfill.py
  - scripts/gdelt_poller.py
  - deploy/systemd/geopol-gdelt-poller.service
  - tests/test_gdelt_poller.py
autonomous: true

must_haves:
  truths:
    - "GDELT poller fetches lastupdate.txt and downloads new export CSV when URL changes"
    - "Events are deduplicated via INSERT OR IGNORE on gdelt_id before graph insertion"
    - "IngestRun rows in PostgreSQL track events_fetched, events_new, events_duplicate, and duration per poll cycle"
    - "On GDELT feed failure, poller retries with exponential backoff (1min -> 2min -> 4min -> max 30min)"
    - "On SIGTERM, poller shuts down cleanly and marks current run as interrupted"
    - "New events are added to the knowledge graph incrementally via add_event_from_db_row (O(N_new))"
    - "When URL-dedup fast path triggers (same lastupdate.txt URL as last poll), events_fetched=0 is a valid successful run, not a failure"
  artifacts:
    - path: "src/ingest/gdelt_poller.py"
      provides: "Async GDELT micro-batch polling daemon"
      min_lines: 200
    - path: "src/ingest/backfill.py"
      provides: "Gap recovery from last successful run on startup"
      min_lines: 40
    - path: "scripts/gdelt_poller.py"
      provides: "Entry point for systemd unit"
      min_lines: 15
    - path: "src/db/models.py"
      provides: "PendingQuestion ORM model + daemon_type column on IngestRun"
      contains: "class PendingQuestion"
    - path: "alembic/versions/20260301_002_pending_questions_and_daemon_type.py"
      provides: "Alembic migration for new table and column"
      contains: "pending_questions"
    - path: "tests/test_gdelt_poller.py"
      provides: "Unit tests for poller logic"
      min_lines: 80
  key_links:
    - from: "src/ingest/gdelt_poller.py"
      to: "src/database/storage.py"
      via: "EventStorage.insert_events() wrapped in asyncio.to_thread()"
      pattern: "asyncio\\.to_thread.*insert_events"
    - from: "src/ingest/gdelt_poller.py"
      to: "src/knowledge_graph/graph_builder.py"
      via: "TemporalKnowledgeGraph.add_event_from_db_row() wrapped in asyncio.to_thread()"
      pattern: "asyncio\\.to_thread.*add_event_from_db_row"
    - from: "src/ingest/gdelt_poller.py"
      to: "src/db/models.py"
      via: "IngestRun ORM rows persisted per poll cycle"
      pattern: "IngestRun"
---

<objective>
Build the GDELT micro-batch polling daemon that fetches 15-minute update feeds, deduplicates events, inserts them into SQLite, updates the knowledge graph incrementally, and records per-run metrics in PostgreSQL. Also adds the PendingQuestion ORM model and daemon_type column needed by the daily pipeline (Plan 04).

Purpose: This is the data lifeline of the system -- without continuous GDELT ingestion, the knowledge graph goes stale and daily forecasts lose relevance. The daemon must be production-grade: graceful shutdown, exponential backoff, gap recovery on restart.

Output: Runnable GDELT poller daemon (`scripts/gdelt_poller.py`), ORM schema additions with Alembic migration, systemd unit file, unit tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-ingest-forecast-pipeline/10-CONTEXT.md
@.planning/phases/10-ingest-forecast-pipeline/10-RESEARCH.md
@src/db/models.py
@src/database/storage.py
@src/knowledge_graph/graph_builder.py
@src/settings.py
@src/db/postgres.py
@alembic/versions/20260301_1443_001_initial_schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Dependencies, ORM schema changes, and Alembic migration</name>
  <files>
    pyproject.toml
    src/settings.py
    src/db/models.py
    alembic/versions/20260301_002_pending_questions_and_daemon_type.py
    src/ingest/__init__.py
  </files>
  <action>
  1. Add new dependencies to pyproject.toml: `aiohttp>=3.9`, `cachetools>=5.3`, `feedparser>=6.0`, `trafilatura>=2.0`. Run `uv sync` to install.

  2. Add Settings fields to `src/settings.py`:
     - `gdelt_poll_interval: int = 900` (15 min in seconds)
     - `gdelt_backfill_on_start: bool = True`
     - `gemini_daily_budget: int = 25` (max questions/day)
     - `redis_url: str` (already exists, verify)
     - `rss_poll_interval_tier1: int = 900` (15 min)
     - `rss_poll_interval_tier2: int = 3600` (1 hour)
     - `rss_article_retention_days: int = 90`

  3. Add `PendingQuestion` model to `src/db/models.py` (per RESEARCH.md spec):
     - id (Integer, PK, autoincrement)
     - question (Text, not null)
     - country_iso (String(3), nullable)
     - horizon_days (Integer, default=21)
     - category (String(32), not null)
     - priority (Integer, default=0)
     - created_at (DateTime(timezone=True), default=_utcnow)
     - status (String(20), default="pending") -- pending | processing | completed

  4. Add `daemon_type` column to `IngestRun` model:
     - `daemon_type: Mapped[str] = mapped_column(String(20), nullable=False, default="gdelt")`
     This distinguishes GDELT vs RSS ingest runs in the same table.

  5. Create Alembic migration that adds the `pending_questions` table and the `daemon_type` column to `ingest_runs`. Use `op.add_column` for the existing table and `op.create_table` for the new one. Downgrade drops both.

  6. Create empty `src/ingest/__init__.py`.
  </action>
  <verify>
  `uv sync` succeeds with new deps. `python -c "from src.db.models import PendingQuestion; print(PendingQuestion.__tablename__)"` prints "pending_questions". `python -c "from src.db.models import IngestRun; print(IngestRun.daemon_type.property.columns[0].default.arg)"` prints "gdelt".
  </verify>
  <done>
  pyproject.toml has 4 new deps installed. Settings has GDELT/RSS polling config. PendingQuestion model exists. IngestRun has daemon_type. Alembic migration file exists with upgrade/downgrade. `src/ingest/` package created.
  </done>
</task>

<task type="auto">
  <name>Task 2: GDELT micro-batch poller daemon with backoff and SIGTERM handling</name>
  <files>
    src/ingest/gdelt_poller.py
    src/ingest/backfill.py
    scripts/gdelt_poller.py
    deploy/systemd/geopol-gdelt-poller.service
  </files>
  <action>
  Build the core GDELT polling daemon following the architecture patterns in RESEARCH.md.

  **src/ingest/gdelt_poller.py** -- the main daemon class:

  1. `GDELTUpdate` dataclass: filesize, md5_hash, url (parsed from lastupdate.txt).

  2. `BackoffStrategy` class: exponential backoff with base=60s, max=1800s (30min), 10% jitter. Methods: `async wait() -> float`, `reset()`.

  3. `GDELTPoller` class:
     - Constructor takes `poll_interval` (default 900s from Settings), `event_storage` (EventStorage), `graph` (TemporalKnowledgeGraph | None).
     - `_shutdown: asyncio.Event` for graceful SIGTERM.
     - `_last_seen_url: str | None` to skip duplicate feeds.
     - `_backoff: BackoffStrategy` for failure recovery.

  4. `async run()` method -- main event loop:
     - Register SIGTERM/SIGINT handlers via `loop.add_signal_handler()`.
     - Call `_backfill_if_needed()` on startup (if Settings.gdelt_backfill_on_start).
     - Loop while not `_shutdown.is_set()`:
       a. Record run start time.
       b. Call `_poll_once()` in try/except.
       c. On success: `_backoff.reset()`, record success IngestRun.
       d. On failure: log warning, record failed IngestRun, await `_backoff.wait()`.
       e. Wait for next poll: `asyncio.wait_for(self._shutdown.wait(), timeout=self.poll_interval)`.
     - On exit: mark current run as "interrupted" if one was in progress.

  5. `async _poll_once()` method:
     - Create `aiohttp.ClientSession`.
     - Fetch `http://data.gdeltproject.org/gdeltv2/lastupdate.txt`.
     - Parse to find the `.export.CSV.zip` line -> `GDELTUpdate`.
     - Compare URL to `_last_seen_url`. If same, log debug "No new data" and return early (with metrics: events_fetched=0, events_new=0, events_duplicate=0).
     - Download the zip, extract CSV, parse via pandas (tab-sep, no header, GDELT_COLUMNS from RESEARCH.md).
     - Convert DataFrame rows to Event objects compatible with `EventStorage.insert_events()`.
     - Use `await asyncio.to_thread(self.event_storage.insert_events, events)` -- do NOT call synchronous code directly in the event loop.
     - Track events_fetched (total rows), events_new (inserted), events_duplicate (fetched - new).
     - If graph is available: for each new event, call `await asyncio.to_thread(self.graph.add_event_from_db_row, row_dict)` -- incremental O(N_new) update per INGEST-02. **IMPORTANT**: Before calling this method, verify it exists on the TemporalKnowledgeGraph instance. If `add_event_from_db_row` does not exist on the graph object, create a thin wrapper method `_add_event_to_graph(row_dict)` that calls `self.graph.add_event(...)` using the appropriate alternative API, and log a warning about the missing method. Do NOT crash if the method signature differs from what RESEARCH.md asserts.
     - Update `_last_seen_url`.
     - Record IngestRun to PostgreSQL via async session.

  6. `_handle_shutdown()` method: sets `_shutdown` event, logs info.

  7. `async _record_run()` method: creates IngestRun with daemon_type="gdelt", started_at, completed_at, status, events_fetched, events_new, events_duplicate, error_message. Uses `get_async_session()` from `src.db.postgres`.

  **CRITICAL: async-to-sync bridge.**
  - `EventStorage.insert_events()` is synchronous SQLite. Wrap in `asyncio.to_thread()`.
  - `TemporalKnowledgeGraph.add_event_from_db_row()` is synchronous. Wrap in `asyncio.to_thread()`.
  - Do NOT attempt to make these async -- that's scope creep per RESEARCH.md pitfall #5.

  **src/ingest/backfill.py**:
  - `async backfill_from_last_run(event_storage, graph)`: query PostgreSQL for most recent successful IngestRun with daemon_type="gdelt". If gap > poll_interval, log warning about missed cycles. The GDELT lastupdate.txt only shows the LATEST update, so we can only fetch current data (no historical backfill from this endpoint). Just log the gap and proceed -- actual backfill from GDELT's historical archive is out of scope for Phase 10.

  **scripts/gdelt_poller.py**:
  - Entry point: parse CLI args (--no-backfill, --poll-interval), create GDELTPoller, call `asyncio.run(poller.run())`.
  - Initialize logging via `setup_logging()` from `src.logging_config`.
  - Initialize PostgreSQL via `init_db()` from `src.db.postgres`.

  **deploy/systemd/geopol-gdelt-poller.service**:
  - Use the template from RESEARCH.md. Type=exec, KillSignal=SIGTERM, TimeoutStopSec=60, Restart=on-failure, RestartSec=30.
  </action>
  <verify>
  `python -c "from src.ingest.gdelt_poller import GDELTPoller, BackoffStrategy; print('OK')"` succeeds.
  `python -c "from src.knowledge_graph.graph_builder import TemporalKnowledgeGraph; assert hasattr(TemporalKnowledgeGraph, 'add_event_from_db_row'), 'method missing -- poller must use fallback wrapper'; print('method exists')"` -- if this fails, confirm the poller's `_add_event_to_graph` wrapper handles it.
  `python scripts/gdelt_poller.py --help` shows usage.
  </verify>
  <done>
  GDELT poller daemon is runnable, handles SIGTERM gracefully, retries with exponential backoff, tracks per-run metrics in IngestRun, and updates knowledge graph incrementally. Backfill module detects gaps on startup. Graph method existence is verified at build time; fallback wrapper used if method signature differs.
  </done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for GDELT poller</name>
  <files>
    tests/test_gdelt_poller.py
  </files>
  <action>
  Write unit tests for the poller's core logic. Mock external I/O (aiohttp, EventStorage, PostgreSQL).

  Tests to include:
  1. **test_parse_lastupdate_txt**: Feed sample lastupdate.txt content, verify `GDELTUpdate` parsing extracts the `.export.CSV.zip` line correctly.
  2. **test_skip_duplicate_url**: Set `_last_seen_url` to a URL, call `_poll_once()` with same URL in lastupdate.txt, verify no download occurs and metrics show 0 events.
  3. **test_backoff_strategy**: Create BackoffStrategy(base=1.0, max_delay=10.0). Call `wait()` 5 times, verify delays are approximately 1, 2, 4, 8, 10 (capped). Call `reset()`, verify next wait is ~1.
  4. **test_record_ingest_run**: Verify IngestRun is created with daemon_type="gdelt", correct status, and event counts.
  5. **test_shutdown_signal**: Create poller, set shutdown event, verify run() exits cleanly.
  6. **test_poll_failure_triggers_backoff**: Mock aiohttp to raise, verify backoff.wait() is called and IngestRun records failure.
  7. **test_incremental_graph_update**: Mock add_event_from_db_row, verify it's called once per new event (not per total event).

  Use `pytest` with `unittest.mock.AsyncMock` for async mocking. Use `aiohttp.test_utils` or `unittest.mock.patch` for HTTP mocking.

  Do NOT make tests that require a running GDELT feed or PostgreSQL -- pure unit tests with mocks.
  </action>
  <verify>
  `uv run pytest tests/test_gdelt_poller.py -v` -- all tests pass, no skips.
  </verify>
  <done>
  7+ unit tests covering lastupdate.txt parsing, URL dedup, backoff strategy, IngestRun recording, graceful shutdown, failure recovery, and incremental graph update. All tests pass without external dependencies.
  </done>
</task>

</tasks>

<verification>
1. `uv sync` installs all new dependencies without errors.
2. `python -c "from src.db.models import PendingQuestion, IngestRun; assert hasattr(IngestRun, 'daemon_type')"` succeeds.
3. `python -c "from src.ingest.gdelt_poller import GDELTPoller; print('import OK')"` succeeds.
4. `python -c "from src.knowledge_graph.graph_builder import TemporalKnowledgeGraph; print('has add_event_from_db_row:', hasattr(TemporalKnowledgeGraph, 'add_event_from_db_row'))"` -- logs whether fallback wrapper is needed.
5. `uv run pytest tests/test_gdelt_poller.py -v` -- all tests pass.
6. `python scripts/gdelt_poller.py --help` shows CLI usage.
7. `deploy/systemd/geopol-gdelt-poller.service` file exists with correct configuration.
</verification>

<success_criteria>
- GDELT poller daemon is fully implemented with async event loop, SIGTERM handling, exponential backoff, and incremental graph updates.
- PendingQuestion ORM model and daemon_type column exist with Alembic migration.
- Settings has all GDELT/RSS polling configuration fields.
- Unit tests validate core poller logic without external dependencies.
- Graph method `add_event_from_db_row` existence is explicitly verified; fallback wrapper handles missing/divergent API.
- URL-dedup fast path producing events_fetched=0 is a valid successful run (not a failure indicator).
- Requirements INGEST-01 through INGEST-05 are covered.
</success_criteria>

<output>
After completion, create `.planning/phases/10-ingest-forecast-pipeline/10-01-SUMMARY.md`
</output>
