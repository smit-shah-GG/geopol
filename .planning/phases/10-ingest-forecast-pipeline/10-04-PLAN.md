---
phase: 10-ingest-forecast-pipeline
plan: 04
type: execute
wave: 2
depends_on: ["10-01", "10-02"]
files_modified:
  - src/pipeline/__init__.py
  - src/pipeline/question_generator.py
  - src/pipeline/daily_forecast.py
  - src/pipeline/outcome_resolver.py
  - src/pipeline/budget_tracker.py
  - src/api/routes/v1/forecasts.py
  - src/api/app.py
  - scripts/daily_forecast.py
  - deploy/systemd/geopol-daily-forecast.service
  - deploy/systemd/geopol-daily-forecast.timer
  - tests/test_daily_pipeline.py
autonomous: true

must_haves:
  truths:
    - "Daily pipeline generates yes/no forecast questions from recent high-significance GDELT events via Gemini"
    - "Daily pipeline runs EnsemblePredictor.predict() for each question and persists results via ForecastService"
    - "When Gemini daily budget is exhausted mid-pipeline, remaining questions are persisted to pending_questions table"
    - "Next day's run prioritizes queued carryover questions before generating fresh ones"
    - "Outcome resolver compares expired predictions against GDELT events and writes OutcomeRecord rows"
    - "GET /api/v1/forecasts/country/{iso} returns real forecasts from PostgreSQL with cache layer"
    - "POST /api/v1/forecasts invokes live EnsemblePredictor with rate limiting and input sanitization"
    - "Consecutive pipeline failures trigger log-based alerts"
  artifacts:
    - path: "src/pipeline/question_generator.py"
      provides: "LLM-based forecast question generation from high-significance events"
      min_lines: 80
    - path: "src/pipeline/daily_forecast.py"
      provides: "Main daily pipeline orchestrator (question gen -> predict -> persist -> resolve)"
      min_lines: 150
    - path: "src/pipeline/outcome_resolver.py"
      provides: "Outcome resolution comparing predictions to GDELT ground truth"
      min_lines: 80
    - path: "src/pipeline/budget_tracker.py"
      provides: "Gemini budget tracking and PendingQuestion queue management"
      min_lines: 60
    - path: "src/api/routes/v1/forecasts.py"
      provides: "Real forecast endpoints replacing mock fixtures, with cache + rate limit + sanitization"
      contains: "ForecastService"
    - path: "scripts/daily_forecast.py"
      provides: "Entry point for systemd timer"
      min_lines: 30
    - path: "tests/test_daily_pipeline.py"
      provides: "Tests for pipeline components"
      min_lines: 100
  key_links:
    - from: "src/pipeline/daily_forecast.py"
      to: "src/forecasting/ensemble_predictor.py"
      via: "EnsemblePredictor.predict() called per question"
      pattern: "predict\\("
    - from: "src/pipeline/daily_forecast.py"
      to: "src/api/services/forecast_service.py"
      via: "ForecastService.persist_forecast() after predict()"
      pattern: "persist_forecast"
    - from: "src/pipeline/daily_forecast.py"
      to: "src/pipeline/budget_tracker.py"
      via: "Budget check before each Gemini call, queue on exhaustion"
      pattern: "budget|PendingQuestion"
    - from: "src/api/routes/v1/forecasts.py"
      to: "src/api/services/cache_service.py"
      via: "ForecastCache.get/set on read endpoints"
      pattern: "cache\\.get|cache\\.set"
    - from: "src/api/routes/v1/forecasts.py"
      to: "src/api/middleware/rate_limit.py"
      via: "check_rate_limit as FastAPI Depends on POST"
      pattern: "check_rate_limit|rate_limit"
    - from: "src/api/routes/v1/forecasts.py"
      to: "src/api/middleware/sanitize.py"
      via: "validate_forecast_question on POST body"
      pattern: "validate_forecast_question|sanitize"
---

<objective>
Build the daily automated forecast pipeline (question generation, prediction, persistence, outcome resolution, budget management) and wire the real API endpoints to serve live forecasts with caching, rate limiting, and sanitization.

Purpose: This is the plan that makes the system autonomous -- daily forecast generation happens without human intervention, past predictions are resolved against ground truth, and external users see real forecast data instead of mock fixtures.

Output: Daily pipeline script (`scripts/daily_forecast.py`), question generator, outcome resolver, budget tracker, updated API routes with real data + hardening, systemd timer, tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-ingest-forecast-pipeline/10-CONTEXT.md
@.planning/phases/10-ingest-forecast-pipeline/10-RESEARCH.md
@.planning/phases/10-ingest-forecast-pipeline/10-01-SUMMARY.md
@.planning/phases/10-ingest-forecast-pipeline/10-02-SUMMARY.md
@src/forecasting/ensemble_predictor.py
@src/api/services/forecast_service.py
@src/api/routes/v1/forecasts.py
@src/api/services/cache_service.py
@src/api/middleware/rate_limit.py
@src/api/middleware/sanitize.py
@src/db/models.py
@src/database/storage.py
@src/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Question generator, budget tracker, and outcome resolver</name>
  <files>
    src/pipeline/__init__.py
    src/pipeline/question_generator.py
    src/pipeline/budget_tracker.py
    src/pipeline/outcome_resolver.py
  </files>
  <action>
  Create `src/pipeline/` package.

  **src/pipeline/question_generator.py**:

  1. `QuestionGenerator` class:
     - Constructor takes `gemini_client` (from `src/forecasting/reasoning_orchestrator.py` -- reuse the existing GeminiClient), `event_storage` (EventStorage).
     - `generate_questions(n_questions: int = 10, min_goldstein: float = 5.0) -> list[GeneratedQuestion]`:
       a. Query EventStorage for recent high-significance events (last 48 hours, sorted by GoldsteinScale descending, min_mentions >= 5).
       b. Format events into the prompt template from RESEARCH.md (QUESTION_GENERATION_PROMPT).
       c. Call Gemini to generate forecast questions as JSON.
       d. Parse JSON response into `GeneratedQuestion` dataclass: question, country_iso, horizon_days, category.
       e. Apply ceiling: cap at `Settings.gemini_daily_budget` (default 25) questions to prevent budget blowout.
       f. Return list of questions.

  2. `GeneratedQuestion` dataclass: question (str), country_iso (str), horizon_days (int), category (str).

  3. Use `asyncio.to_thread()` to call synchronous EventStorage methods.

  **src/pipeline/budget_tracker.py**:

  1. `BudgetTracker` class:
     - Constructor takes `async_session_factory` (from postgres.py), `daily_limit` (from Settings.gemini_daily_budget).
     - `async check_budget() -> int`: returns remaining Gemini calls for today. Uses Redis counter via `gemini_budget_remaining()` from rate_limit.py if Redis available, else counts today's predictions in PostgreSQL.
     - `async increment() -> int`: increment Gemini usage. Returns current count.
     - `async queue_question(question: GeneratedQuestion) -> None`: persist to PendingQuestion table with status="pending".
     - `async dequeue_pending(limit: int = 10) -> list[PendingQuestion]`: fetch pending questions ordered by priority DESC, created_at ASC. Mark as "processing".
     - `async mark_completed(question_id: int) -> None`: update status to "completed".

  2. If Redis is unavailable, budget tracking falls back to counting today's Prediction rows in PostgreSQL (more expensive but functional).

  **src/pipeline/outcome_resolver.py**:

  1. `OutcomeResolver` class:
     - Constructor takes `async_session_factory`, `event_storage`, `gemini_client` (optional, for LLM-based resolution).
     - `async resolve_expired_predictions(batch_size: int = 20) -> list[OutcomeRecord]`:
       a. Query PostgreSQL for predictions past their `expires_at` that have no corresponding OutcomeRecord.
       b. For each expired prediction:
          - Query GDELT events in the prediction's time window (created_at to expires_at).
          - Use Gemini to assess: "Given this prediction and these events, did the predicted outcome occur? Respond with a float 0.0-1.0." (per RESEARCH.md recommendation -- LLM-based resolution is more accurate than keyword matching).
          - If Gemini unavailable or budget exhausted: use simple heuristic (actor+event_code matching) as fallback.
       c. Create OutcomeRecord with: prediction_id, outcome (0.0-1.0), resolution_date, resolution_method ("gemini" or "heuristic"), evidence_gdelt_ids (list of matching event IDs), notes.
       d. Persist OutcomeRecord.
     - Cap at `batch_size` resolutions per run to avoid burning Gemini budget.

  2. `_heuristic_resolve(prediction: Prediction, events: list[Event]) -> tuple[float, list[str]]`:
     - Extract entities from prediction.entities JSON.
     - Search events for actor code matches.
     - Return (outcome_score, matching_event_ids).
     - This is the fallback when Gemini is unavailable.
  </action>
  <verify>
  `python -c "from src.pipeline.question_generator import QuestionGenerator, GeneratedQuestion; print('OK')"` succeeds.
  `python -c "from src.pipeline.budget_tracker import BudgetTracker; print('OK')"` succeeds.
  `python -c "from src.pipeline.outcome_resolver import OutcomeResolver; print('OK')"` succeeds.
  </verify>
  <done>
  Question generator feeds high-significance events to Gemini and produces forecast questions. Budget tracker manages Gemini daily limits with PendingQuestion queue for carryover. Outcome resolver compares expired predictions to GDELT ground truth (LLM-based with heuristic fallback).
  </done>
</task>

<task type="auto">
  <name>Task 2: Daily pipeline orchestrator, entry point, and systemd timer</name>
  <files>
    src/pipeline/daily_forecast.py
    scripts/daily_forecast.py
    deploy/systemd/geopol-daily-forecast.service
    deploy/systemd/geopol-daily-forecast.timer
  </files>
  <action>
  **src/pipeline/daily_forecast.py**:

  Main orchestrator that runs the complete daily cycle.

  1. `DailyPipeline` class:
     - Constructor takes: question_generator, budget_tracker, outcome_resolver, ensemble_predictor (EnsemblePredictor), async_session_factory.
     - `_consecutive_failures: int = 0` for failure alerting.

  2. `async run_daily() -> PipelineResult`:
     Complete daily cycle:

     a. **Phase 1: Dequeue carryover** -- `budget_tracker.dequeue_pending()` to get questions from yesterday's budget exhaustion.

     b. **Phase 2: Generate fresh questions** -- If budget remaining after carryover, `question_generator.generate_questions()`. Combine with carryover (carryover first, fresh after).

     c. **Phase 3: Predict and persist** -- For each question:
        - Check budget: `budget_tracker.check_budget()`. If exhausted, `budget_tracker.queue_question()` and break.
        - Call `EnsemblePredictor.predict()` via `asyncio.to_thread()` (it's synchronous).
        - Call `ForecastService.persist_forecast()` to store in PostgreSQL.
        - `budget_tracker.increment()`.
        - `budget_tracker.mark_completed()` if from carryover queue.
        - Log each forecast: question, probability, country_iso.

     d. **Phase 4: Resolve outcomes** -- `outcome_resolver.resolve_expired_predictions()`. Log count of resolved predictions.

     e. Record pipeline result: questions_generated, forecasts_produced, questions_queued (budget overflow), outcomes_resolved, errors.

  3. `async run_with_retry(max_retries: int = 2) -> PipelineResult`:
     - Try `run_daily()`. On success: reset `_consecutive_failures`.
     - On failure: increment `_consecutive_failures`, log error.
     - If `_consecutive_failures >= 2`: emit alert log (CRITICAL level): "ALERT: {n} consecutive daily pipeline failures".
     - Retry up to max_retries with 5-minute delay between retries.

  4. `PipelineResult` dataclass: questions_generated, forecasts_produced, questions_queued, outcomes_resolved, errors (list[str]), duration_seconds, success (bool).

  **scripts/daily_forecast.py**:
  - Entry point for systemd timer.
  - Initialize logging, PostgreSQL, Redis.
  - Construct all pipeline components: EventStorage, TemporalKnowledgeGraph, RAGPipeline, GeminiClient, ReasoningOrchestrator, TKGPredictor, EnsemblePredictor, ForecastService, QuestionGenerator, BudgetTracker, OutcomeResolver.
  - Use `asyncio.to_thread()` for synchronous component initialization.
  - Run `DailyPipeline.run_with_retry()`.
  - Exit with code 0 on success, 1 on failure.
  - CLI args: --max-questions (override), --skip-outcomes (skip phase 4), --dry-run (generate questions but don't predict).

  **deploy/systemd/geopol-daily-forecast.service**:
  - Type=oneshot (runs once, not a daemon).
  - ExecStart: `python -m scripts.daily_forecast`

  **deploy/systemd/geopol-daily-forecast.timer**:
  - OnCalendar=*-*-* 06:00:00 (daily at 6am per AUTO-01).
  - Persistent=true (run if system was off at 06:00).
  - RandomizedDelaySec=300 (5 min jitter).
  </action>
  <verify>
  `python -c "from src.pipeline.daily_forecast import DailyPipeline, PipelineResult; print('OK')"` succeeds.
  `python scripts/daily_forecast.py --help` shows usage with --dry-run, --max-questions, --skip-outcomes.
  `ls deploy/systemd/geopol-daily-forecast.*` shows both .service and .timer files.
  </verify>
  <done>
  Daily pipeline orchestrates the full forecast cycle: dequeue carryover -> generate questions -> predict+persist -> resolve outcomes. Handles budget exhaustion by queuing overflow. Consecutive failure alerting. systemd timer runs at 06:00 daily.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire real API endpoints with cache, rate limit, and sanitization + tests</name>
  <files>
    src/api/routes/v1/forecasts.py
    src/api/app.py
    tests/test_daily_pipeline.py
  </files>
  <action>
  **src/api/routes/v1/forecasts.py** -- update existing routes to serve real data:

  1. `GET /forecasts/country/{iso_code}`:
     - Check ForecastCache first (cache key: `country:{iso_code.upper()}`).
     - If cache hit: return cached response.
     - If cache miss: query PostgreSQL via `ForecastService.get_forecasts_by_country()`.
     - If PostgreSQL returns results: cache them (TTL: SUMMARY_TTL=3600) and return.
     - If PostgreSQL returns empty: fall back to mock fixtures (preserving Phase 9 behavior for development).
     - Add `cache: ForecastCache = Depends(get_cache)` to function signature.

  2. `GET /forecasts/top`:
     - Check ForecastCache (cache key: `top:{limit}`).
     - If miss: query PostgreSQL for top N forecasts ordered by probability DESC.
     - Add a `get_top_forecasts` method to ForecastService for this query.
     - Cache result with SUMMARY_TTL.
     - Fall back to mock fixtures if no real data.

  3. `GET /forecasts/{forecast_id}`:
     - Already queries PostgreSQL first (Phase 9 wired this). Add cache layer:
       - Check ForecastCache (cache key: `forecast:{forecast_id}`).
       - If miss: query PostgreSQL, cache with FULL_FORECAST_TTL.
       - Fall back to mock fixtures.

  4. `POST /forecasts`:
     - Replace mock implementation with real EnsemblePredictor invocation.
     - Add dependencies: `Depends(check_rate_limit)` for rate limiting, call `validate_forecast_question()` on the question text.
     - Flow: sanitize input -> check rate limit -> call `EnsemblePredictor.predict()` via `asyncio.to_thread()` -> persist via `ForecastService` -> cache result -> return ForecastResponse.
     - If Gemini budget exhausted: return 429 with detail "Forecast generation budget exhausted for today."
     - Wrap predict() in try/except: on failure, return 500 with generic error (no internals leaked via sanitize_error_response).
     - Add `redis: Redis = Depends(get_redis)` for rate limit and budget checking.

  5. Keep mock fixture routes accessible at `/api/v1/fixtures/*` (create a separate fixture router if needed, or just keep the `_get_forecast_cache()` function for fixture fallback).

  **src/api/app.py** -- update lifespan:
  - Add Redis initialization in lifespan startup (call `get_redis()`).
  - Add Redis close in lifespan shutdown (call `_close_redis()` from deps.py).
  - Do NOT remove existing startup/shutdown logic.

  **tests/test_daily_pipeline.py** -- tests for pipeline components and route wiring:

  1. **test_question_generator_formats_prompt**: Mock Gemini response, verify questions parsed correctly.
  2. **test_question_generator_caps_at_budget**: Set budget to 5, verify max 5 questions returned.
  3. **test_budget_tracker_queues_on_exhaustion**: Exhaust budget, verify PendingQuestion created.
  4. **test_budget_tracker_dequeues_carryover_first**: Add pending questions, verify they're returned before fresh generation.
  5. **test_outcome_resolver_heuristic**: Provide prediction + matching GDELT events, verify outcome > 0.
  6. **test_pipeline_result_on_success**: Mock all components, run pipeline, verify PipelineResult.success=True.
  7. **test_pipeline_consecutive_failure_alert**: Force two failures, verify CRITICAL log emitted.
  8. **test_forecast_route_cache_hit**: Mock cache.get returning data, verify PostgreSQL NOT queried.
  9. **test_forecast_route_cache_miss_populates**: Mock cache miss, verify cache.set called after PostgreSQL query.
  10. **test_post_forecast_rate_limited**: Mock rate limit exceeded, verify 429 response.
  11. **test_post_forecast_sanitized**: Submit injection attempt, verify 400 response.
  12. **test_post_forecast_budget_exhausted**: Mock budget at 0, verify 429 response with budget message.

  Use `pytest` with `unittest.mock` and FastAPI's `TestClient` (via `httpx.AsyncClient` for async routes).
  </action>
  <verify>
  `uv run pytest tests/test_daily_pipeline.py -v` -- all 12 tests pass.
  `python -c "from src.pipeline.daily_forecast import DailyPipeline; print('OK')"` succeeds.
  `python -c "from src.api.routes.v1.forecasts import router; print(len(router.routes))"` shows route count.
  </verify>
  <done>
  Daily pipeline orchestrates full forecast cycle autonomously. API routes serve real forecasts with cache layer, rate limiting, and input sanitization. Mock fixtures preserved as fallback. Budget exhaustion queues questions for next day. Consecutive failure alerting active. 12 tests cover pipeline components and route wiring.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.pipeline.daily_forecast import DailyPipeline; print('OK')"` succeeds.
2. `python scripts/daily_forecast.py --help` shows usage.
3. `uv run pytest tests/test_daily_pipeline.py -v` -- all tests pass.
4. `ls deploy/systemd/geopol-daily-forecast.*` shows .service and .timer.
5. GET /forecasts/country/SY returns real data (or mock fallback) with cache headers.
6. POST /forecasts with injection attempt returns 400.
7. POST /forecasts with exhausted budget returns 429.
</verification>

<success_criteria>
- Daily pipeline generates questions, produces forecasts, resolves outcomes, and manages budget autonomously.
- API routes serve real forecasts with three-tier cache, rate limiting, and input sanitization.
- Budget exhaustion gracefully queues overflow questions for next day.
- Consecutive failure alerting protects against silent pipeline death.
- Mock fixtures preserved as development fallback.
- Requirements AUTO-01 through AUTO-05 are fully covered.
- API routes satisfy API-04, API-05, API-06 by integrating components from Plan 02.
</success_criteria>

<output>
After completion, create `.planning/phases/10-ingest-forecast-pipeline/10-04-SUMMARY.md`
</output>
