---
phase: 01-data-foundation
plan: 02
type: execute
---

<objective>
Design and implement event storage with deduplication for temporal knowledge graph construction.

Purpose: Create persistent storage that prevents duplicate events and enables efficient TKG building.
Output: SQLite database with event schema, storage layer, and deduplication system.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@src/gdelt_client.py
@src/fetch_events.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Design SQLite schema for event storage</name>
  <files>src/database/schema.sql, src/database/__init__.py, src/database/models.py</files>
  <action>Create schema.sql with events table: id (INTEGER PRIMARY KEY), gdelt_id (TEXT UNIQUE), event_date (TEXT), actor1_code (TEXT), actor2_code (TEXT), event_code (TEXT), quad_class (INTEGER), goldstein_scale (REAL), num_mentions (INTEGER), num_sources (INTEGER), tone (REAL), url (TEXT), title (TEXT), domain (TEXT), content_hash (TEXT), time_window (TEXT), created_at (TEXT), raw_json (TEXT). Add indexes on: event_date, quad_class, content_hash + time_window (for deduplication), actor codes. Create models.py with Event dataclass matching schema fields. Include methods for to_dict() and from_gdelt_row() conversion.</action>
  <verify>sqlite3 data/events.db < src/database/schema.sql creates database without errors, python -c "from src.database.models import Event; print('Models loaded')" works</verify>
  <done>Database schema created, indexes defined, Event model implemented</done>
</task>

<task type="auto">
  <name>Task 2: Implement storage layer with batch operations</name>
  <files>src/database/storage.py, src/database/connection.py</files>
  <action>Create connection.py with get_connection() using context manager for safe connection handling, connection pooling with sqlite3 default settings. Create storage.py with EventStorage class: __init__(db_path="data/events.db"), init_database() to create tables if not exist, insert_events(events: List[Event]) with batch inserts using executemany, get_events(start_date, end_date, quad_classes=[1,4]) with filtering, get_event_count() for statistics. Use transactions for batch operations. Include proper error handling and logging for database operations.</action>
  <verify>python -c "from src.database.storage import EventStorage; storage = EventStorage(); storage.init_database(); print(f'Event count: {storage.get_event_count()}')" outputs count without errors</verify>
  <done>Storage layer implemented with batch operations and safe connection handling</done>
</task>

<task type="auto">
  <name>Task 3: Create deduplication system using content hashing</name>
  <files>src/deduplication.py, test_deduplication.py</files>
  <action>Create deduplication.py with deduplicate_events(events_df) function: generates content_hash using hashlib.md5 from concatenated actor1_code + actor2_code + event_code + location fields, creates time_window by flooring event timestamp to hour, removes duplicates based on content_hash + time_window combination keeping first occurrence, logs duplicate statistics. Add is_duplicate(event, storage) to check against existing database entries. Create test_deduplication.py that fetches sample events, applies deduplication, shows before/after counts, inserts to database, verifies no duplicates inserted on second run.</action>
  <verify>python test_deduplication.py shows deduplication working (before > after count), second run inserts 0 new events</verify>
  <done>Deduplication system prevents duplicate events, content hashing works correctly</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `sqlite3 data/events.db ".schema"` shows events table with indexes
- [ ] `python test_deduplication.py` successfully deduplicates events
- [ ] Batch inserts handle 1000+ events efficiently
- [ ] No duplicate events in database after multiple runs
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- SQLite database created with proper schema
- Deduplication prevents redundant events (addressing 20% redundancy issue)
- Batch operations work efficiently
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-02-SUMMARY.md`:

# Phase 1 Plan 2: Event Storage Schema Summary

**Implemented persistent storage with deduplication for GDELT events**

## Accomplishments

- Created SQLite schema optimized for TKG construction
- Built storage layer with batch operations and transactions
- Implemented content-hash deduplication system

## Files Created/Modified

- `src/database/schema.sql` - Event table schema with indexes
- `src/database/storage.py` - EventStorage class with batch operations
- `src/database/models.py` - Event dataclass for type safety
- `src/deduplication.py` - Content hashing and duplicate detection
- `test_deduplication.py` - Deduplication verification script

## Decisions Made

- SQLite for simplicity and portability (can migrate to PostgreSQL later)
- Content hash from actor+event+location fields for deduplication
- Hour-based time windows for duplicate detection

## Issues Encountered

None expected for database setup

## Next Step

Ready for 01-03-PLAN.md (Sampling and Filtering)
</output>