---
phase: 01-data-foundation
plan: 03
type: execute
---

<objective>
Implement intelligent sampling and filtering strategies for compute-constrained processing.

Purpose: Manage GDELT's 500K-1M daily article volume through strategic sampling while preserving signal quality.
Output: Sampling system with QuadClass filtering, GDELT100 quality filtering, and monitoring metrics.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@.planning/phases/01-data-foundation/01-02-SUMMARY.md
@src/gdelt_client.py
@src/database/storage.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement QuadClass and quality filtering</name>
  <files>src/filtering.py, src/constants.py</files>
  <action>Create constants.py with QUADCLASS_VERBAL_COOPERATION = 1, QUADCLASS_MATERIAL_CONFLICT = 4, GDELT100_THRESHOLD = 100 (mentions), MIN_GOLDSTEIN_CONFLICT = -5.0, TONE_RANGE_DIPLOMATIC = (-2, 2). Create filtering.py with filter_by_quadclass(events_df, classes=[1,4]) selecting only specified QuadClasses, filter_high_confidence(events_df, min_mentions=100) for GDELT100 quality filtering, filter_by_tone_and_goldstein(events_df) applying tone ranges for conflict/diplomatic classification, combine_filters(events_df) chaining all filters with logging of reduction at each step. Include statistics tracking: events per QuadClass, average mentions, tone distribution.</action>
  <verify>python -c "from src.filtering import combine_filters; import pandas as pd; df = pd.DataFrame({'QuadClass': [1,2,3,4], 'NumMentions': [50,100,150,200]}); filtered = combine_filters(df); print(f'Filtered: {len(filtered)} events')" runs without error</verify>
  <done>Filtering functions implemented, QuadClass 1 & 4 selection works, GDELT100 threshold applied</done>
</task>

<task type="auto">
  <name>Task 2: Create stratified sampling for volume management</name>
  <files>src/sampling.py, test_sampling.py</files>
  <action>Create sampling.py with sample_events_stratified(events_df, max_events=10000): calculate QuadClass distribution in input, allocate samples proportionally to each class, within each class prioritize by NumMentions (descending), ensure minimum representation (10%) for minority class, return sampled DataFrame maintaining temporal order. Add adaptive_sampling(events_df, compute_budget) that adjusts sample size based on available resources. Create test_sampling.py demonstrating sampling on large dataset (generate 50K fake events), verify class distribution preserved, confirm high-mention events prioritized.</action>
  <verify>python test_sampling.py shows stratified sampling preserving QuadClass ratios, output size matches target</verify>
  <done>Stratified sampling maintains class balance, prioritizes high-confidence events</done>
</task>

<task type="auto">
  <name>Task 3: Build monitoring and data quality metrics</name>
  <files>src/monitoring.py, src/pipeline.py, run_pipeline.py</files>
  <action>Create monitoring.py with DataQualityMonitor class tracking: event counts (raw, filtered, sampled), duplicate rate percentage, QuadClass distribution, average tone by class, processing time per stage, estimated accuracy (using 55% baseline from research). Add log_metrics() outputting to console and metrics.json. Create pipeline.py orchestrating full flow: fetch → filter → deduplicate → sample → store → monitor. Create run_pipeline.py as main entry point: accepts date range arguments, runs complete pipeline with monitoring, outputs summary statistics, saves metrics to data/metrics/. Include error recovery and checkpoint saving.</action>
  <verify>python run_pipeline.py --date 2026-01-09 processes events and outputs metrics showing filtering/sampling statistics</verify>
  <done>Complete pipeline runs with monitoring, metrics tracked at each stage, quality indicators visible</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python run_pipeline.py --date 2026-01-09` completes full pipeline
- [ ] Metrics show deduplication reducing events by ~20%
- [ ] Sampling maintains QuadClass distribution
- [ ] GDELT100 filtering improves signal quality
- [ ] Pipeline handles errors gracefully
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Complete data pipeline operational
- Sampling strategies manage volume effectively
- Quality metrics provide visibility into data characteristics
- Phase 1 complete: Data foundation established
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-03-SUMMARY.md`:

# Phase 1 Plan 3: Sampling and Filtering Summary

**Implemented intelligent sampling and quality filtering for compute-constrained processing**

## Accomplishments

- Built QuadClass and GDELT100 quality filtering
- Created stratified sampling preserving event distribution
- Implemented monitoring with data quality metrics
- Assembled complete data pipeline from fetch to storage

## Files Created/Modified

- `src/filtering.py` - Multi-stage filtering system
- `src/sampling.py` - Stratified sampling for volume control
- `src/monitoring.py` - Data quality metrics tracking
- `src/pipeline.py` - Complete data processing pipeline
- `run_pipeline.py` - Main entry point for execution

## Decisions Made

- GDELT100 threshold (100+ mentions) for quality vs quantity tradeoff
- Stratified sampling to preserve QuadClass distribution
- Monitoring at each pipeline stage for observability

## Issues Encountered

Expected: Rate limiting may require tuning delays

## Next Step

Phase 1 complete, ready for Phase 2: Knowledge Graph Engine
</output>