# v2.0 Hybrid Architecture — CANCELLED

**Cancelled:** 2026-02-14
**Reason:** Trading a frontier-class reasoning engine (Gemini) for a consumer-grade local model (4-bit Llama2-7B on RTX 3060 12GB) does not produce a net quality improvement. The engineering effort (6 phases, ~12 plans) targets uncertain accuracy gains while degrading the system's strongest component: LLM reasoning quality.

**Decision rationale:**
- Gemini dramatically outperforms 4-bit Llama2-7B for nuanced geopolitical reasoning
- RTX 3060 12GB VRAM is insufficient to comfortably run quantized LLM + JAX TKG
- 40-60% accuracy improvement target was aspirational, not evidence-based
- Gemini improves for free with each model generation
- API costs are negligible for the system's query volume
- The real-world base rates and scenario decomposition already work well with Gemini (v1.1.1 verification confirmed)

**Preserved for potential future revisitation** (e.g., when hardware constraints change or competitive local models emerge).

---

## Original Milestone Definition

**Goal:** Replace post-hoc 60/40 ensemble with deep token-space integration where TKG embeddings project directly into Llama2-7B token space, targeting 40-60% accuracy improvement.

**Target features:**
- TGL-LLM style adapter architecture (RGCN -> GRU -> learned adapter -> Llama2-7B)
- Self-hosted Llama2-7B inference (4-bit quantized for RTX 3060 12GB)
- Context-quality-aware adaptive weighting (when deep integration unavailable)
- TKG algorithm upgrade (RE-GCN -> HisMatch for ~6% MRR gain)
- A/B comparison framework (v1.1 Gemini vs v2.0 Llama-TGL)

**Constraints:**
- RTX 3060 12GB VRAM — adapter training feasible, full fine-tuning not feasible
- Frozen Llama backbone — only adapter layers trained
- Must validate gains before deprecating Gemini path

**Success criteria:** Any measurable accuracy improvement over v1.1 baseline.

---

## Original Requirements (23 total)

### Deep Integration Core

- **DEEP-01**: System projects TKG embeddings (200-dim) into Llama2-7B token space (4096-dim) via learned adapter layers
- **DEEP-02**: System performs temporal tokenization across T=5-7 historical snapshots for temporal context
- **DEEP-03**: System runs Llama2-7B inference with 4-bit NF4 quantization fitting RTX 3060 12GB VRAM
- **DEEP-04**: System trains LoRA adapters on frozen Llama2-7B backbone for domain-specific fine-tuning
- **DEEP-05**: System bridges JAX RE-GCN embeddings to PyTorch Llama via zero-copy transfer (DLPack or NumPy)
- **DEEP-06**: LLM reasons jointly over graph structure and text, not just post-hoc combination
- **DEEP-07**: System predicts 42+ CAMEO relation types (multi-class), not just binary outcomes
- **DEEP-08**: System assesses context quality and adjusts confidence based on graph signal strength

### TKG Encoder

- **TKG-01**: RE-GCN encoder extracts temporal embedding sequences (T snapshots) for adapter input
- **TKG-02**: System caches entity embeddings to amortize RE-GCN encoding cost across predictions
- **TKG-03**: System upgrades TKG algorithm from RE-GCN (40.4% MRR) to HisMatch (~46.4% MRR)
- **TKG-04**: System adapts temporal window length based on event density in query region

### Validation & Comparison

- **VAL-01**: System runs v1.1 Gemini and v2.0 Llama-TGL in parallel on same test questions
- **VAL-02**: System computes accuracy metrics (precision, recall, F1) on held-out geopolitical events
- **VAL-03**: System computes Brier scores for both v1.1 and v2.0 predictions
- **VAL-04**: System reports per-relation accuracy breakdown across CAMEO taxonomy
- **VAL-05**: System generates calibration analysis (ECE, reliability diagrams) for both models
- **VAL-06**: System benchmarks inference latency (end-to-end prediction time) for both models

### Dashboard

- **DASH-01**: Streamlit dashboard displays comparison metrics (accuracy, Brier, latency) with charts
- **DASH-02**: Streamlit dashboard shows live side-by-side predictions from v1.1 and v2.0 models
- **DASH-03**: Dashboard updates metrics as new predictions are evaluated

### Migration

- **MIG-01**: Gemini-based v1.1 path remains operational throughout v2.0 development
- **MIG-02**: System supports manual fallback to v1.1 if v2.0 predictions fail or degrade

---

## Original Phases (9-14)

### Phase 9: Environment Setup & Data Preparation
**Goal**: JAX/PyTorch memory coordination works, GDELT data prepared for deep integration training
**Depends on**: Phase 8 (graph infrastructure)
**Requirements**: DEEP-05, MIG-01
**Success Criteria**:
  1. JAX and PyTorch coexist in same process without OOM from memory pre-allocation conflict
  2. DLPack zero-copy tensor conversion transfers embeddings JAX->PyTorch without CPU round-trip
  3. CAMEO relation codes map to training-friendly taxonomy (20 root categories)
  4. v1.1 Gemini forecasting path remains fully operational during v2.0 development

### Phase 10: TKG Encoder & Adapter Architecture
**Goal**: TKG encoder upgraded to HisMatch, adapter layers project 200-dim embeddings to 4096-dim Llama token space
**Depends on**: Phase 9 (environment ready)
**Requirements**: TKG-01, TKG-03, TKG-04, DEEP-01, DEEP-03, DEEP-04
**Success Criteria**:
  1. RE-GCN replaced with HisMatch encoder achieving ~46% MRR on GDELT test set
  2. Entity/relation adapters project (batch, 200) embeddings to (batch, 4096) with correct layer norm
  3. Llama2-7B loads with 4-bit NF4 quantization using <4GB VRAM
  4. LoRA adapters configured on q_proj/v_proj with frozen backbone (verify trainable param count)
  5. Temporal window length adapts based on event density in query region

### Phase 11: Temporal Tokenizer & Llama Integration
**Goal**: Graph embeddings tokenized across T snapshots and injected as soft prompts into Llama generation
**Depends on**: Phase 10 (adapters ready)
**Requirements**: DEEP-02
**Success Criteria**:
  1. Temporal tokenizer sequences T=5-7 graph snapshots as soft tokens concatenated with text query
  2. Graph token positions tracked for embedding injection during LLM forward pass
  3. Llama generates coherent text with injected graph tokens (sanity check: removing graph tokens changes output)

### Phase 12: Two-Stage Training Pipeline
**Goal**: Adapters and LoRA trained with cross-modal alignment, model predicts 42+ CAMEO relations with context-aware confidence
**Depends on**: Phase 11 (integration ready)
**Requirements**: DEEP-06, DEEP-07, DEEP-08
**Success Criteria**:
  1. Stage 1 training achieves cross-modal alignment on 100K high-quality samples
  2. Stage 2 training generalizes across diverse CAMEO relations with stratified sampling
  3. Model predicts 42+ CAMEO relation types with per-class F1 >0.3 on held-out test
  4. Model outputs confidence scores that vary based on graph signal strength
  5. LLM reasoning references graph structure in explanations

### Phase 13: Evaluation & Calibration
**Goal**: v2.0 TGL-LLM validated against v1.1 baseline with calibrated probabilities
**Depends on**: Phase 12 (trained model)
**Requirements**: VAL-01 through VAL-06
**Success Criteria**:
  1. A/B test harness runs v1.1 Gemini and v2.0 TGL-LLM on identical held-out questions
  2. v2.0 shows measurable accuracy improvement over v1.1
  3. Brier scores computed; v2.0 calibration does not regress (ECE <0.15)
  4. Per-relation accuracy breakdown available for all 42+ CAMEO categories
  5. Inference latency benchmarked end-to-end

### Phase 14: Dashboard & Integration
**Goal**: Streamlit dashboard displays comparison metrics, v2.0 integrated as primary forecaster with v1.1 fallback
**Depends on**: Phase 13 (evaluation complete)
**Requirements**: DASH-01 through DASH-03, TKG-02, MIG-02
**Success Criteria**:
  1. Streamlit dashboard displays accuracy, Brier, and latency metrics with charts
  2. Dashboard shows live side-by-side predictions from v1.1 and v2.0 models
  3. Dashboard updates metrics as new predictions are evaluated
  4. Entity embeddings cached to amortize encoding cost
  5. Manual fallback to v1.1 available

---

## Traceability (at time of cancellation)

| Requirement | Phase | Status |
|-------------|-------|--------|
| DEEP-01 | Phase 10 | Cancelled |
| DEEP-02 | Phase 11 | Cancelled |
| DEEP-03 | Phase 10 | Cancelled |
| DEEP-04 | Phase 10 | Cancelled |
| DEEP-05 | Phase 9 | Cancelled |
| DEEP-06 | Phase 12 | Cancelled |
| DEEP-07 | Phase 12 | Cancelled |
| DEEP-08 | Phase 12 | Cancelled |
| TKG-01 | Phase 10 | Cancelled |
| TKG-02 | Phase 14 | Cancelled |
| TKG-03 | Phase 10 | Cancelled |
| TKG-04 | Phase 10 | Cancelled |
| VAL-01 | Phase 13 | Cancelled |
| VAL-02 | Phase 13 | Cancelled |
| VAL-03 | Phase 13 | Cancelled |
| VAL-04 | Phase 13 | Cancelled |
| VAL-05 | Phase 13 | Cancelled |
| VAL-06 | Phase 13 | Cancelled |
| DASH-01 | Phase 14 | Cancelled |
| DASH-02 | Phase 14 | Cancelled |
| DASH-03 | Phase 14 | Cancelled |
| MIG-01 | Phase 9 | Cancelled |
| MIG-02 | Phase 14 | Cancelled |

---

## CAMEO Taxonomy Decisions (partially discussed before cancellation)

During the Phase 9 discussion, these decisions were made before the pivot:
- **Granularity**: Hierarchical 20 root categories + 2-digit sub-types as secondary labels
- **Rebalancing**: Natural distribution (preserve real-world base rates)
- **Goldstein scores**: Include as continuous feature alongside categorical CAMEO codes
- **Multi-label events**: Multi-label encoding (multi-hot vector for events with multiple CAMEO codes)

These decisions may be relevant if CAMEO taxonomy work is included in a future milestone.

---
*Archived: 2026-02-14*
